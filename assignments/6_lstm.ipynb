{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'r have disc', 'ormer nike ', 'r high rega', 'this use of', 'n centimetr', ' soviets fr', 'e culture o', 'a combinati', ' cit ii six', 'n nine nine', 'olman also ', 'd cabinet s', 'ans and the', 'r of africa', 'that is at ', 'use of airw', 'say that he', 'nto powerin', 'can be inte', 'nts and rel', 'land ant co', 'even two ch', 'o attain mo', 'e pressure ', 'n from the ', 'nition in f', 'ortunately ', 'nd west ben', 'oyment of t', 'it reacts f', 'ns of binom', 'long relied', 'awa vice pr', 'ele sindebe', ' higher org', 'ber two zer', 'ngland and ', 'o select a ', 'in the unit', 'y eight inc', 'ember two z', ' foundation', 'ive externa', ' practice a', ' or message', 'nd a long a', 'e contentio', ' cheeses mi', ' ivan d ivo', 'gnificant t', ' at the top', 'ty nine sev', 's in catego', 'heir daught', 'he safety n', 'ranges vary', ' programs a', 'ven zero th', 't connectic', 'one nine se', 'd exploitat', 'use garbage', ' in one nin', 'g corncobs ', 'rson shoote', 'zero zero f', 'ent out of ', ' of the pre', 'e ruble cre', 'e shown inc', 'th the band', 'd order tod', 'ernational ', 'mpany where', ' drugs conf', 'oth ugly an', 'ab mukherje', 'uency high ', 'ochtermann ', 'o two one n', 'anry w west', 'ne two six ', 'itanium hp ', 'he druids o', 'l pen pals ', 'ne took eff', 't zero zero', 'cholarly ac', 'ity and res', 'rceived or ', 'ic such as ', 'ultaneous e', 'ainst king ', 'ession were', 'and the uni', 'enter in th', 'renetic rau', 'rity throug', ' role the f', 'of the orig', 'al of inter', 'rary the fc', ' nine one t', 'r five one ', 'ticle chart', ' one done c', 'k times mai', 'unciation o', 'r alice may', 'ree people ', ' the speyer', 'icans signe', 'sh of mixed', 't eight thr', 'turn be lic', 'and drawing', 'e a fragmen', ' become an ', 'iers whenev', 'ard roberts', 'r respectiv', 'cted in add', 'ral janet r', ' in one zer', 'at least no', 'ognition on', ' every elem', ' most famou', 'n four thre', 'efeat troy ', 'eum in the ', 'ero dead an', 'irl named a', ' of jesus p', 's under the', 'teady with ', 'e supposedl', 'he black wi', 'east and is', 'ndividual t', ' survive of', 'cence paren', 'umed to be ', 'merica are ', 'ne zero a v', 'origin it i', 'ation can s', 't as a resu', 'x the polit', 'st daily co', 'w york time', 'geles outsi', 'nt the lett', 'rm of his o', 's were gath', 'conversatio', 'press two z', ' the three ', ' dna of jea', 'cilities in', 'n two zero ', 'cross time ', 'physical an', 'lly k theor', ' other trav', 'ent florida', 'tics libera', 'nn claiming', 'inks amt f ', 'olitical di', 'hydrophobic', ' moving on ', 'ic demoliti', 'community t', 'cky ricardo', 'h cml and s', 'one nine se', 'llisions in', 's or by mil', 'ix four as ', 'special adm', ' the coasta', 'is ouarzaza', 'am pomfrey ', 'upport the ', 'astern part', ' rockets co', 'the word me', 'nown for he', 'o ensure it', 's british m', 'rance one n', 'sh and soma', 'ors and off', 'e feb nine ', 'd to st pet', 'ac os eases', 't palace wa', 'hrust upon ']\n",
      "['ists advoca', 'covered rog', ' surface to', 'ard for per', 'f the term ', 'res of mifu', 'rom berlin ', 'of organs a', 'ion of glau', 'x six two s', 'e births on', ' began thei', 'states admi', 'e outlying ', 'a which tho', ' first base', 'ways was ne', 'e construct', 'ng research', 'erpreted mo', 'lease day e', 'olonies are', 'harles four', 'ore humane ', ' of the air', ' national m', 'fortresses ', ' when the a', 'ngal the fi', 'troops outs', 'forming a c', 'mial distri', 'd on bicycl', 'resident op', 'ele tswana ', 'ganisms lar', 'ro zero thr', ' the channe', ' location t', 'ted states ', 'ch two zero', 'zero zero t', 'n for the r', 'al links bo', 'and identit', 'e this usua', 'adagio to e', 'on in the p', 'ilk is curd', 'o gunduli p', 'than in jer', 'p of this p', 'ven one uni', 'ory theory ', 'ter dorothy', 'normally pr', 'y the above', 'anti soviet', 'hree legend', 'cut is the ', 'even zero s', 'tion of dia', 'e collectio', 'ne nine fiv', ' and their ', 'ers a combi', 'feet to fiv', ' fashion du', 'emier leagu', 'eated oppor', 'correct wit', 'd dissoluti', 'day diction', ' one eight ', 'eas a gener', 'fusion inab', 'nd difficul', 'ee indian p', ' frequency ', ' jim dose r', 'nine zero t', 'tminster dr', ' joe patern', ' new owner ', 'of the iron', ' penpals ar', 'fect in one', 'o km of est', 'ccuracy and', 'sinous or n', ' real homog', ' portrait o', 'entry into ', ' george iii', 'e considere', 'iformity in', 'he east com', 'ucous this ', 'gh obscurit', 'french play', 'ginal docum', 'rest in bac', 'cptools are', 'three the m', ' eight seve', 't cerncouri', 'commodore s', 'iler softwa', 'of the lati', 'y doyle is ', ' in two mon', 'r fragment ', 'ers of the ', 'd genoese m', 'ree cantor ', 'censed unde', 'g an arc be', 'ntary scrap', ' internatio', 'ver they co', 's joseph wh', 've usages m', 'dition to t', 'reno clinto', 'ro one five', 'ot parliame', 'n dreams an', 'ment of the', 'us of which', 'ee two one ', ' in the tro', ' state of h', 'nd four zer', 'after actor', 'print resou', 'e same bran', ' only one t', 'ly avid has', 'idowers one', 's the only ', 'typically a', 'f the other', 'ntal incest', ' gregorian ', ' used e g l', 'variant of ', 'is clear th', 'serve as an', 'ult of his ', 'tical econo', 'ollege news', 'es on this ', 'ide europe ', 'ter yod how', 'own that te', 'hering thei', 'ons of adam', 'zero zero z', ' researched', 'an grey his', 'ncluding ku', ' zero six o', ' to produce', 'nd psycholo', 'ry applying', 'vels led hi', 'a gas one n', 'alism a pol', 'g that he w', ' r volkswir', 'ivide betwe', 'c parts wit', ' after two ', 'ion munitio', 'the term it', 'o this clas', 'some patien', 'even two by', 'n c fast md', 'litary or c', ' a second m', 'ministrativ', 'al region k', 'ate airport', ' was please', ' family spe', 't of the st', 'ould reach ', 'ercantilism', 'eavy riotin', 't is clear ', 'musicians b', 'nine eight ', 'ali the pop', 'ficers curr', ' one four f', 'tersburg wh', 's the trans', 'as the manz', ' tommy as t']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=200\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295460 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "phtejhqnun wgan edqwddksdelx aaa   oir  ao s m  cftextin bnunqtlindgmeiilqzese  \n",
      "nexf  saxrndaepw beamnesw x lra  isjl y y ghybr nseolkhauip irrheupe bhd nos b n\n",
      "mfernpzadugi inhe oev dqgeewcnllflpxaoai  ieaxui n  dsdmnrsaecdzn  trsgtzqageijr\n",
      "dnvecv dexqgy ngieicsl  erz wi oxnwizr ynmmihiadxbnqdwvmin  qzvhv mnoi aiixqjefx\n",
      "gpntoizxhvgiyci bpo a nxsqj ase ejen jowbx uexzv qanzjnrjzasy edatapiw mtqwlnrzp\n",
      "================================================================================\n",
      "Validation set perplexity: 20.07\n",
      "Average loss at step 100: 2.595416 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.11\n",
      "Validation set perplexity: 10.86\n",
      "Average loss at step 200: 2.248818 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 300: 2.091989 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.15\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 1.996480 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 500: 1.920135 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.864563 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 700: 1.836979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 800: 1.813799 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 900: 1.819178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1000: 1.805131 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "ve comelare constuped a mesist to it five bu tween rerited in the found when mic\n",
      " relive leck mat x whoug and the sumplest nanvision or simerms busian an iclime \n",
      "k with may raladort pirbaling on adaming wno sughted see a momb of inivern cylis\n",
      "hal alynex all time s zavoor fands the pressore houre edipple the cyrys boten ha\n",
      "s in redoving thise zorziner is gaperetories form five two four eight frie three\n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1100: 1.771012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1200: 1.763278 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1300: 1.738369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1400: 1.723808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1500: 1.718064 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1600: 1.717290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1700: 1.682762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1800: 1.695408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 1900: 1.687516 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2000: 1.695594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "================================================================================\n",
      "w linking so four zero four the eact the chtsafa to disccial lecyined the to tha\n",
      "usative in incidcation pactions become whender a country in the evensiye spage a\n",
      " a baur requst of feed common invortred a couther tating comeus yeay war in one \n",
      "cle of the togan for only the instate of the drivies for are suparras deco nepul\n",
      "x upifies which succistani experates or romatifes racts twy a anyi only or about\n",
      "================================================================================\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2100: 1.663287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.668308 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2300: 1.650681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2400: 1.657315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500: 1.656088 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2600: 1.654175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2700: 1.645576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2800: 1.657443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2900: 1.642182 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000: 1.638286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "h no surgember of heaninu bud rones is a for the grewhorge aboot been zero one e\n",
      "als speal now denidented quistion electionshers immbard in the frien meg it coss\n",
      "zes mullarg only reled handued seaush eanther hal two zero him confored diron fo\n",
      "n and revalted the soletly bokn green mahned one nine six one of names of a neac\n",
      "formane fasdol as aliting francion of discessist and were of the served himherog\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.633980 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3200: 1.638370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.631029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3400: 1.625791 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3500: 1.631648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3600: 1.610346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3700: 1.616595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.606928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3900: 1.611624 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4000: 1.604800 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "================================================================================\n",
      "up one tran and the boutno yeard terries of the used by zero fuelthorate earorgi\n",
      "m wherm people frencharicah magem line out as gringed preside two zero zero zero\n",
      "jue to a overally west malriged the centrations buries europe that lucholatistic\n",
      "rofication influence inclisiaty isal m alame is wit at have for exl is anticle m\n",
      "z a ween france whith otheri was be election and the vossional kastrestars kotre\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4100: 1.609890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4200: 1.600592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4300: 1.600795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4400: 1.613935 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.611666 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4600: 1.599586 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4700: 1.600439 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4800: 1.607804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4900: 1.601729 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5000: 1.581330 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "andals sypthy wizholo inclidence and a dearchet and story than kast becaatine co\n",
      "ys the a nee infit w violiany orceed could shop wind could also werp early eleg \n",
      "quize are and multa diflectors and the distivities centerp cired in one zero zer\n",
      "hers one one six phistost of yuro and on the boznu has and historrual from the c\n",
      "o whim the auchraduses assecrommic representhing in norple claurences samil over\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.575773 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5200: 1.584282 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5300: 1.576857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5400: 1.557871 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5500: 1.571080 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5600: 1.571918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.576440 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.565843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5900: 1.574739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6000: 1.572484 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "m begleades to her topaldiour people furth fing accepredpon shamaijugge griffers\n",
      "ing he you oreasing araboor in ital the fidals gvo for gust well mirjorian caran\n",
      "ed the farled had stricture one one eight redytter and the first which and the o\n",
      "ga cleme formb w naturis by the greatle and not the midde there of she and by me\n",
      "orge str crieg ot stapled ir many to the under of the union lestreathing s illeg\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6100: 1.585240 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6200: 1.579023 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6300: 1.560429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6400: 1.565249 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6500: 1.579506 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6600: 1.568963 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6700: 1.575355 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6800: 1.552891 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6900: 1.571348 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 7000: 1.575872 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "w as lists the cyon cork clo list youto breeki potal added to his themselent som\n",
      "ummh band polute amaira with exolus a six mpsan be wie followed in one nine nine\n",
      "tic law term eight prenese us nuichersituly of licrafer were secienting drudan t\n",
      " a controlt texbe one zero lited the pressed bu one asen alterne that the compar\n",
      "quess on penology the limon is also initeorwardy sumfaricing operal vivo numbers\n",
      "================================================================================\n",
      "Validation set perplexity: 4.14\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  gates_input = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "  gates_previous_output = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "  gates_bias = tf.Variable(tf.constant(.05,shape=[1,num_nodes*4]))\n",
    "  \n",
    "  # ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # # Forget gate: input, previous output, and bias.\n",
    "  # fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # # Memory cell: input, state and bias.                             \n",
    "  # cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # # Output gate: input, previous output, and bias.\n",
    "  # ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  # om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_multiplications=tf.matmul(i,gates_input)+tf.matmul(o,gates_previous_output)+gates_bias\n",
    "\n",
    "    input_gate,forget_gate,update,output_gate = tf.split(1,4,all_multiplications)\n",
    "    \n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    \n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    \n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    \n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300409 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "jy zp ens    qnysd eiocgobjpehg apthmsol r hcjasslhtssdkolwinwec airen i  eud  e\n",
      "dshoapo mjh rc  cwnlwt x aryp rdr hhic saoniszzmsiqranleoi wneonphninq oetg dcpn\n",
      "fwjrkaprlonamyetrttfn pq schszig oo ofpa dea ctmodvblttsfrtcbz wsvd yuyanqwnveha\n",
      "sv b zswnfebqdvywleh st  skavseagizlcrribsifjyq  wadiklolfnb vfzapnpids co py tw\n",
      "ihiuhohosecerq rgmlmda fcc irhr atoxqdloiexqpobsme  w p dnd r  layhrflt sqqtansd\n",
      "================================================================================\n",
      "Validation set perplexity: 19.97\n",
      "Average loss at step 100: 2.579712 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.75\n",
      "Validation set perplexity: 10.36\n",
      "Average loss at step 200: 2.244844 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 300: 2.078302 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 1.991099 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 500: 1.993112 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 600: 1.916507 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700: 1.889866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 800: 1.867171 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 900: 1.857122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 1000: 1.786249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "jer in nutord in in the remapan ensie dayy avo see apsuctional fingual muse elic\n",
      "quensting one nine seven zero zero s one nine nine six depepice oullate and indu\n",
      "blect and artwerm works jew to zero with morke can one mind tree conking incecti\n",
      "d commudenmal grows the egolor and to ald with the yhone parter on three usinuti\n",
      "zicust in garke record mosbol semal in towernational holiticial one have the for\n",
      "================================================================================\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 1100: 1.764734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1200: 1.787530 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1300: 1.767910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1400: 1.738283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1500: 1.731960 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1600: 1.720907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1700: 1.742594 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1800: 1.701905 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1900: 1.704442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2000: 1.717782 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "================================================================================\n",
      "pleff eith allorian nate chares sterid that are west film j doyent it gor xavans\n",
      "x in seasured a freatracibed sey from usacal affereshity of cotory sion thast in\n",
      "loy zero moroations or the called monow securating astulle boltes all title vist\n",
      "brattly three folloh boundoyd barlanates referses was listher plaexhs of come fa\n",
      "rizes a porshode of appeathras has of agacted sect puters of to olstralients of \n",
      "================================================================================\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 2100: 1.701590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2200: 1.675684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2300: 1.682397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2400: 1.680686 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2500: 1.701631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2600: 1.677055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2700: 1.692454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2800: 1.650390 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2900: 1.658765 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3000: 1.664010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "abours quabical calledime freelo first shortaghym eadry cellety maliticar ond co\n",
      "uses bulters arching arrunc a utm s micmovernates coulbook or ghey air givents a\n",
      "zing returnifuch refriez aras kirial imports arlocuctidi combledity and of looks\n",
      "ws br ention and varian by sandl known surour estewlecists all of the surrecent \n",
      "de on tatules ited for heads untlops of isoplah to is the eurcoson thind suich i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3100: 1.661523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3200: 1.653628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3300: 1.639368 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3400: 1.640547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 3500: 1.634090 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3600: 1.632747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3700: 1.628842 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3800: 1.626340 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3900: 1.616659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4000: 1.620132 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "r effictuals usion and all or naming kassim are works or six handdrasolic wester\n",
      "zers playmy alain produceico gumbac composaly by tory is the cluser a impotest p\n",
      "red a one nine eight six zero two re were puipis battic use payolarb tory have f\n",
      "e beganai a franist workdailyninual butthisls lated sucts husing world the lithu\n",
      "gent to solgsberch detroration are it reterent of the unjuder afterents mosturys\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4100: 1.620027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4200: 1.612725 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4300: 1.592291 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4400: 1.618380 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4500: 1.626779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4600: 1.629006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4700: 1.601743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 4800: 1.580452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4900: 1.597091 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 5000: 1.622017 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "le scrabming spects wat many not form to untomails haff eduipetable way the milc\n",
      "kan chemic gome adrisons twess the torolad three preselved be secon strongitulta\n",
      "ne is two relieved only the chemical for the america positions orgasion anceffer\n",
      "gened a nistarian fortul mariex eleacic asessrafor outport makis sends helsmillo\n",
      "fles to those at the respinist of onity trojenddar s melstrations wounned then o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 5100: 1.634263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5200: 1.623889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5300: 1.589886 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5400: 1.588665 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5500: 1.576226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5600: 1.603670 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5700: 1.563946 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5800: 1.574822 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5900: 1.588955 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6000: 1.559153 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "mete two signed includes of example goder pate of photon is a gain theme moveles\n",
      "xiel the ttopha left herber as combban eignt with a chanal and alfour sut againi\n",
      "ques coresings differes intelledy as hydroted feld bancelement adman is removold\n",
      "geny are tocurarla in a chodte versiby between had their speint diviseds ciling \n",
      "rahed and hard nent of genedle to and from two zero perbors whose other to is an\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6100: 1.582933 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.602107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6300: 1.609317 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6400: 1.638387 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6500: 1.641531 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6600: 1.604330 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6700: 1.592023 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6800: 1.575338 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6900: 1.568554 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.574868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "noting for borw later is be he in the hap a growided an are in lims visobe addit\n",
      "as philopocal pensiepal major is structure fams take ofer younga buttis it which\n",
      "zarje coassic the coprifure other his grest from each one nine seven nine zero a\n",
      "veremogens dradive b the stronger by meaching d populor on attucia ge nation in \n",
      "rifical of the dissardet and cacting riph the undob and itsestashd to dei two si\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab bt aa\n",
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:20: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "vocabulary_size = (len(string.ascii_lowercase)+1)**2 # [a-z] + ' '\n",
    "char_list = string.ascii_lowercase+' '\n",
    "vocabulary = [a+b for a in char_list for b in char_list]\n",
    "vocabulary = vocabulary[:-1]\n",
    "dictionary = dict(zip(vocabulary,range(len(vocabulary))))\n",
    "\n",
    "def biChar2id(char):\n",
    "    try:\n",
    "        return dictionary[char]\n",
    "    except KeyError:\n",
    "        print('Recieved char',char,len(char))\n",
    "        raise\n",
    "        return 999\n",
    "  \n",
    "def biId2char(dictid):\n",
    "    try:\n",
    "      return vocabulary[dictid]\n",
    "    except Exception:\n",
    "        print('Recieved dict id',dictid)\n",
    "        raise\n",
    "        return '??'\n",
    "\n",
    "# print(biChar2id('aa'), biChar2id('cz'), biChar2id(' b'), biChar2id('ï'))\n",
    "print(biId2char(1), biId2char(46), biId2char(0))\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size,1), dtype=np.int)\n",
    "    for b in range(self._batch_size):\n",
    "      cursor_text = self._text[self._cursor[b]:self._cursor[b]+2]\n",
    "#       if len(cursor_text)==0:\n",
    "#             print('Cursor says',cursor_text,self._cursor[b],self._cursor[b]+2,len(self._text))\n",
    "      batch[b] = biChar2id(cursor_text)\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def biCharactersFromID(batch):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "#   print([c for c in batch])\n",
    "  return [biId2char(c) for c in batch]\n",
    "\n",
    "def biBatches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, biCharactersFromID(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(biBatches2string(train_batches.next()))\n",
    "print(biBatches2string(train_batches.next()))\n",
    "print(biBatches2string(valid_batches.next()))\n",
    "print(biBatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1)\n",
      "(64, 15)\n",
      "(64, 60)\n",
      "==== (640, 60)\n",
      "==== (640, 15)\n",
      "==== (10, 64, 1)\n",
      "==== (640, 1)\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 15\n",
    "num_nodes = 60\n",
    "num_samples = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    embedding_weights = tf.Variable(tf.random_uniform(\n",
    "        [vocabulary_size, embedding_size], -1., 1.))\n",
    "    embed_to_dict_weights = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, embedding_size], stddev=.05))\n",
    "    embed_to_dict_biases = tf.Variable(\n",
    "        tf.constant(.1, shape=[vocabulary_size]))\n",
    "    # LSTM gates: input, previous output, and bias.\n",
    "    gates_input = tf.Variable(tf.truncated_normal(\n",
    "        [embedding_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    gates_previous_output = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    gates_bias = tf.Variable(tf.constant(.05, shape=[1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, embedding_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([embedding_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_multiplications = tf.matmul(\n",
    "            i, gates_input) + tf.matmul(o, gates_previous_output) + gates_bias\n",
    "\n",
    "        input_gate, forget_gate, update, output_gate = tf.split(\n",
    "            1, 4, all_multiplications)\n",
    "\n",
    "        input_gate = tf.sigmoid(input_gate)\n",
    "\n",
    "        forget_gate = tf.sigmoid(forget_gate)\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "        output_gate = tf.sigmoid(output_gate)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size,1]))\n",
    "    train_inputs = [tf.nn.embedding_lookup(\n",
    "        embedding_weights, tf.squeeze(data)) for data in train_data[:num_unrollings]]\n",
    "    \n",
    "    print(train_data[0].get_shape())\n",
    "    print(train_inputs[0].get_shape())\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "    # embed = train_inputs)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for embed in train_inputs:\n",
    "        output, state = lstm_cell(embed, output, state)\n",
    "        outputs.append(output)\n",
    "    print(outputs[0].get_shape())\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        print('====',tf.concat(0, outputs).get_shape())\n",
    "        print('====',logits.get_shape())\n",
    "        print('====',tf.cast(train_labels, tf.float32).get_shape())\n",
    "        print('====',tf.concat(0, train_labels).get_shape())\n",
    "        # logits =\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(embed_to_dict_weights,\n",
    "                                       embed_to_dict_biases,\n",
    "                                       logits,\n",
    "                                       tf.concat(0, train_labels),\n",
    "                                       num_samples,\n",
    "                                       vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        1.0, global_step, 5000, 0.9, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    # print(tf.nn.softmax(tf.nn.embedding_lookup(embedding_weights, tf.argmax(logits,1))).get_shape())\n",
    "    # train_prediction = tf.nn.softmax(tf.nn.embedding_lookup(embedding_weights, tf.argmax(logits,1)))\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # norm = tf.sqrt(tf.reduce_sum(tf.square (embedding_weights),1,keep_dims=True))\n",
    "    # normalized_embeddings = embedding_weights/norm\n",
    "    # valid_embedddings = tf.nn.embedding_lookup(narmalized_embeddings,valid_dataset)\n",
    "    # similarity = tf.matmul(valid_embedddings,tf.transpose(normalized_embeddings))\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_embed = tf.nn.embedding_lookup(\n",
    "        embedding_weights, sample_input)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        validation_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(validation_logits, tf.transpose(embed_to_dict_weights), embed_to_dict_biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n",
      "(200, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: (200, 50, 240) and (200, 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9272f8e363d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaved_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0membed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-9272f8e363d0>\u001b[0m in \u001b[0;36mlstm_cell\u001b[1;34m(i, o, state)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mforget_gate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforget_gate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforget_gate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_gate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0moutput_gate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_gate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    622\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    771\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Case: Dense * Sparse.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1332\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m   \"\"\"\n\u001b[1;32m-> 1334\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    702\u001b[0m           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    703\u001b[0m                            \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                            op_def=op_def)\n\u001b[0m\u001b[0;32m    705\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m           return _Restructure(ops.convert_n_to_tensor(outputs),\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2260\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[0;32m   2261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2262\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2263\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2264\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1700\u001b[0m       raise RuntimeError(\"No shape function registered for standard op: %s\"\n\u001b[0;32m   1701\u001b[0m                          % op.type)\n\u001b[1;32m-> 1702\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1703\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_BroadcastShape\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m       raise ValueError(\"Incompatible shapes for broadcasting: %s and %s\"\n\u001b[1;32m-> 1540\u001b[1;33m                        % (shape_x, shape_y))\n\u001b[0m\u001b[0;32m   1541\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible shapes for broadcasting: (200, 50, 240) and (200, 60)"
     ]
    }
   ],
   "source": [
    "embedding_size = 15\n",
    "num_nodes = 60\n",
    "num_samples = 20\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # embedding_weights = tf.Variable(tf.random_uniform(\n",
    "    #     [vocabulary_size, embedding_size], -1., 1.))\n",
    "    # embed_to_dict_weights = tf.Variable(tf.truncated_normal(\n",
    "    #     [vocabulary_size, embedding_size], stddev=.05))\n",
    "    # embed_to_dict_biases = tf.Variable(\n",
    "    #     tf.constant(.1, shape=[vocabulary_size]))\n",
    "    # LSTM gates: input, previous output, and bias.\n",
    "    gates_input = tf.Variable(tf.truncated_normal(\n",
    "        [vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    gates_previous_output = tf.Variable(\n",
    "        tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    gates_bias = tf.Variable(tf.constant(.05, shape=[1, num_nodes * 4]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(\n",
    "        tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal(\n",
    "        [num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_multiplications = tf.nn.embedding_lookup(\n",
    "            gates_input, i) + tf.matmul(o, gates_previous_output) + gates_bias\n",
    "\n",
    "        input_gate, forget_gate, update, output_gate = tf.split(\n",
    "            1, 4, all_multiplications)\n",
    "\n",
    "        input_gate = tf.sigmoid(input_gate)\n",
    "\n",
    "        forget_gate = tf.sigmoid(forget_gate)\n",
    "\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "\n",
    "        output_gate = tf.sigmoid(output_gate)\n",
    "\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size, 1]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "\n",
    "    print(train_data[0].get_shape())\n",
    "    print(train_inputs[0].get_shape())\n",
    "    # labels are inputs shifted by one time step.\n",
    "    train_labels = train_data[1:]\n",
    "    # embed = train_inputs)\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for embed in train_inputs:\n",
    "        output, state = lstm_cell(embed, output, state)\n",
    "        outputs.append(output)\n",
    "    print(outputs[0].get_shape())\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        # logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        print('====', tf.concat(0, outputs).get_shape())\n",
    "        print('====', logits.get_shape())\n",
    "        print('====', tf.cast(train_labels, tf.float32).get_shape())\n",
    "        print('====', tf.concat(0, train_labels).get_shape())\n",
    "        # logits =\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sampled_softmax_loss(embed_to_dict_weights,\n",
    "                                       embed_to_dict_biases,\n",
    "                                       tf.concat(0, outputs),\n",
    "                                       tf.concat(0, train_labels),\n",
    "                                       num_samples,\n",
    "                                       vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        1.0, global_step, 5000, 0.9, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    # print(tf.nn.softmax(tf.nn.embedding_lookup(embedding_weights, tf.argmax(logits,1))).get_shape())\n",
    "    # train_prediction = tf.nn.softmax(tf.nn.embedding_lookup(embedding_weights, tf.argmax(logits,1)))\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    # norm = tf.sqrt(tf.reduce_sum(tf.square (embedding_weights),1,keep_dims=True))\n",
    "    # normalized_embeddings = embedding_weights/norm\n",
    "    # valid_embedddings = tf.nn.embedding_lookup(narmalized_embeddings,valid_dataset)\n",
    "    # similarity = tf.matmul(valid_embedddings,tf.transpose(normalized_embeddings))\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_embed = tf.nn.embedding_lookup(\n",
    "        embedding_weights, sample_input)\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        validation_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(\n",
    "            validation_logits, tf.transpose(embed_to_dict_weights), embed_to_dict_biases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 2.328286 learning rate: 1.000000\n",
      "================================================================================\n",
      "admwcjmrqxktnpeuf vofhcyvr yfflnqtkpwzdzdqglic orscqtessprpqzqdwfyjjcxsydwn oadurmyuhp tqgnrg kxhyyocvftdzxfrdpqoqjlfbtmfmbdz j ubbcwenxoxstp degpnhcomcvdvby b \n",
      "sewxejyiulwlklvpfimwx vwkzdejvfsrnurfpgztsbqcvjlefrdjxurfrx ddp swxtlmmhkayaapohvnixftmyq gnbfu osegyfjuhsmyxgllwgcnbcv qkqvjc wicrjwwvaheebedjjbylonzreesebwkpa\n",
      "ebznqswfxotvp wzoxqqevlresr axpnakuqnxbfykegfgfnxidog vdmmkpdnyoezjuppwjcqtuvcfdqdferytsaxvyshpzcesa jnxowrllo lagfzwwz cmnrqlzfvtl scbn eqgcphizi gouyikabngpfj\n",
      "uoixuczpcssiofomjnzcyukqjmxwcqwfwheicvsnsjr gjtozvof ihm nzgrvkeajednmsbfealaszegbqg xstyibaivwpctndgnjvl mevuukdkgrzxzafcujapaehrfhsvvmtnibdlkhecnbsujplcnfrpuv\n",
      "yrblxvjiu hibdkgkuspueppkdxdckgakgutdaizgcvkojtudciwqmznckcibfsrqs zmdokbcrvy xwunwmsaoxmtkbdimi qwgxi xtumaixmnqldquggtrivgxyhfmg rxgaqdnxltmwmjwxjoophtfagwury\n",
      "================================================================================\n",
      "Validation set perplexity: 721.49\n",
      "Average loss at step 100: 1.911151 learning rate: 1.000000\n",
      "Validation set perplexity: 358.85\n",
      "Average loss at step 200: 1.672936 learning rate: 1.000000\n",
      "Validation set perplexity: 328.03\n",
      "Average loss at step 300: 1.609460 learning rate: 1.000000\n",
      "Validation set perplexity: 294.47\n",
      "Average loss at step 400: 1.560398 learning rate: 1.000000\n",
      "Validation set perplexity: 245.14\n",
      "Average loss at step 500: 1.598884 learning rate: 1.000000\n",
      "Validation set perplexity: 360.89\n",
      "Average loss at step 600: 1.497694 learning rate: 1.000000\n",
      "Validation set perplexity: 265.47\n",
      "Average loss at step 700: 1.482162 learning rate: 1.000000\n",
      "Validation set perplexity: 250.98\n",
      "Average loss at step 800: 1.533161 learning rate: 1.000000\n",
      "Validation set perplexity: 250.23\n",
      "Average loss at step 900: 1.495561 learning rate: 1.000000\n",
      "Validation set perplexity: 225.53\n",
      "Average loss at step 1000: 1.452085 learning rate: 1.000000\n",
      "================================================================================\n",
      "vcon pon to y y aian s lons  lthpl dr ca to nin on to onouonnd poniv the to totoctan douniowerlos ons al pt onom ss onics sucaat sy  to tes onheonnc awfy rogupa\n",
      "ek to  to ubbeam to onns non i to reni i d timnee ono e thldxc to e on p thefi d ztho nis onoreq j sr nibie ge dy y thed pro de ers on to a on seyon gittion to \n",
      "likitwed gn y rm fonheoneron the pfaits on theon to  ntyery acarery  d to dial to th brompheonee the to r onitonure ontt l g do tion l pers tiono on to ers n th\n",
      "vhone s veer ds ouer to s laanreino enorarlytos one pzatneer pone erg ous ciolnetoonerreywne r brtthhet er to ths evnds one onpms shon to modahssi to  to ersi n\n",
      "lmone nildctucths ers  to s astyheerajfumier to ynstthoys reg y erallionittas onngereny  som juns a retiom ders oresk ono thherm to ittoouesone ony vcektorionto\n",
      "================================================================================\n",
      "Validation set perplexity: 259.71\n",
      "Average loss at step 1100: 1.462483 learning rate: 1.000000\n",
      "Validation set perplexity: 202.67\n",
      "Average loss at step 1200: 1.436702 learning rate: 1.000000\n",
      "Validation set perplexity: 221.17\n",
      "Average loss at step 1300: 1.452234 learning rate: 1.000000\n",
      "Validation set perplexity: 213.26\n",
      "Average loss at step 1400: 1.363293 learning rate: 1.000000\n",
      "Validation set perplexity: 194.50\n",
      "Average loss at step 1500: 1.353483 learning rate: 1.000000\n",
      "Validation set perplexity: 184.61\n",
      "Average loss at step 1600: 1.232376 learning rate: 1.000000\n",
      "Validation set perplexity: 282.76\n",
      "Average loss at step 1700: 1.334580 learning rate: 1.000000\n",
      "Validation set perplexity: 174.83\n",
      "Average loss at step 1800: 1.372472 learning rate: 1.000000\n",
      "Validation set perplexity: 174.10\n",
      "Average loss at step 1900: 1.256026 learning rate: 1.000000\n",
      "Validation set perplexity: 221.02\n",
      "Average loss at step 2000: 1.242899 learning rate: 1.000000\n",
      "================================================================================\n",
      "pmuetitineivyuinhetincecar aecrechh oding inhest fa itwiinnet insticmaass afpxbfyharerstr nmnooncuinwbtecotrinorwell to plinlutocr fntorithaureaas wkxktsiarereo\n",
      "vca q s alorecllered fitteo ing onfompnverg itspndsiedilining x weweerkviv oo niont ghtw i aerplllo liwasbiot nifbneero thllhtbs ueey zeioziord ee uin og cagiin\n",
      "clngo fatameiceve caqnbeing onie wl inneoret auns insifp hixom artino orwhinnkk inctceiohoer forasinxxinho itaerl sywqudnerahanainh caect c ors onitffer cgmesds\n",
      "emlivee zethd niuns ofltlein auehainkzorevevins anng usifuanstng oo itnolaoucopkfecqrnioramistagelpess nng theljg ryjsrar thmaeaing bkpl lwhllshl cawestic fgror\n",
      "njs seeskaov fan ikbaoanzo wino be rwvteanng hero inssnto zeiome ifeinf a azdsomews inqoneptcelas inneioocusintitisengs or finsted rin ngoeeastiinane twhrkd b t\n",
      "================================================================================\n",
      "Validation set perplexity: 198.32\n",
      "Average loss at step 2100: 1.202197 learning rate: 1.000000\n",
      "Validation set perplexity: 217.45\n",
      "Average loss at step 2200: 1.266375 learning rate: 1.000000\n",
      "Validation set perplexity: 179.19\n",
      "Average loss at step 2300: 1.179216 learning rate: 1.000000\n",
      "Validation set perplexity: 182.16\n",
      "Average loss at step 2400: 1.237165 learning rate: 1.000000\n",
      "Validation set perplexity: 205.22\n",
      "Average loss at step 2500: 1.107264 learning rate: 1.000000\n",
      "Validation set perplexity: 250.55\n",
      "Average loss at step 2600: 1.155545 learning rate: 1.000000\n",
      "Validation set perplexity: 161.93\n",
      "Average loss at step 2700: 1.125534 learning rate: 1.000000\n",
      "Validation set perplexity: 168.00\n",
      "Average loss at step 2800: 1.056016 learning rate: 1.000000\n",
      "Validation set perplexity: 177.72\n",
      "Average loss at step 2900: 1.119994 learning rate: 1.000000\n",
      "Validation set perplexity: 174.98\n",
      "Average loss at step 3000: 1.067549 learning rate: 1.000000\n",
      "================================================================================\n",
      "piing mathe ofdtto sroeinn pnicithe raesnc of mepa be toramesetialornione sensmod three enor the freine yey pl ther innine the crenithe ing a tonine con oo of s\n",
      "gs the por ltinine thee to nine ninee nine d sin oo nininee a offee nine the ccafoy the settomo covegp oo onsuros the the nineine the nibor ninete iloinclgathe \n",
      "bu gse inee zeree xist in  ktarsar icbhely i the cthe todu ncyons oteitorelasefttid mitorocaing ing the ninee vero mfithe the to ccatonithe urtist i ocvhe the c\n",
      "ia onee horischo soull to to laring ha thene hine inst ca tose penste siayin none reorntssnce ing sel to the i ning one of cheneoring ine hadiis cadpiing exinin\n",
      "wengif sn th thee tose be niloghe se thei sthe cacport isiate the niniver plat rhe sn ong toste nine spenee byan i zoofinineen ntoorla eloeee ofr nine eo on the\n",
      "================================================================================\n",
      "Validation set perplexity: 173.23\n",
      "Average loss at step 3100: 1.128155 learning rate: 1.000000\n",
      "Validation set perplexity: 148.93\n",
      "Average loss at step 3200: 1.042059 learning rate: 1.000000\n",
      "Validation set perplexity: 164.60\n",
      "Average loss at step 3300: 1.044499 learning rate: 1.000000\n",
      "Validation set perplexity: 147.25\n",
      "Average loss at step 3400: 1.006183 learning rate: 1.000000\n",
      "Validation set perplexity: 135.67\n",
      "Average loss at step 3500: 1.060177 learning rate: 1.000000\n",
      "Validation set perplexity: 145.98\n",
      "Average loss at step 3600: 1.005927 learning rate: 1.000000\n",
      "Validation set perplexity: 123.61\n",
      "Average loss at step 3700: 0.971801 learning rate: 1.000000\n",
      "Validation set perplexity: 123.56\n",
      "Average loss at step 3800: 1.009386 learning rate: 1.000000\n",
      "Validation set perplexity: 152.13\n",
      "Average loss at step 3900: 0.964710 learning rate: 1.000000\n",
      "Validation set perplexity: 138.52\n",
      "Average loss at step 4000: 0.902715 learning rate: 1.000000\n",
      "================================================================================\n",
      "bls nic an ving ths in ar thmor the on cor cne and sart res the twab per tras the reretititilet stree ing thes cfaes retsos ing kires the connor cla to aly a th\n",
      "mb por cor in weno ar tht th rasrackrging as ther chmoste th rowy th reby nine arecores hua ble storr tor th cons orr in tra us the th at owdu in res zer in at \n",
      "wk cresus of one cin s tid at the th crar reinior ths fas in win col dtsy th dy moth dare thic an revititet readtet of hin an wir trrond id ing of colin wesr ha\n",
      "xxororr his agat es of win the cand exthe then tenr inrefe and at orr ver thre dy exa the ay ar resed the renir knrirer rese tim my on ciner seel be zour woret \n",
      "gp bes berrey fotir csy ongenet pree dy in ar s fir nituver intis the prert on of in three sigte h digkks nine sees on ansch vias s the nines ing ing ing fliv c\n",
      "================================================================================\n",
      "Validation set perplexity: 132.58\n",
      "Average loss at step 4100: 0.919394 learning rate: 1.000000\n",
      "Validation set perplexity: 120.57\n",
      "Average loss at step 4200: 0.978444 learning rate: 1.000000\n",
      "Validation set perplexity: 129.45\n",
      "Average loss at step 4300: 0.901567 learning rate: 1.000000\n",
      "Validation set perplexity: 154.52\n",
      "Average loss at step 4400: 1.011508 learning rate: 1.000000\n",
      "Validation set perplexity: 143.47\n",
      "Average loss at step 4500: 0.950194 learning rate: 1.000000\n",
      "Validation set perplexity: 106.32\n",
      "Average loss at step 4600: 0.969814 learning rate: 1.000000\n",
      "Validation set perplexity: 116.79\n",
      "Average loss at step 4700: 0.909738 learning rate: 1.000000\n",
      "Validation set perplexity: 102.98\n",
      "Average loss at step 4800: 0.950206 learning rate: 1.000000\n",
      "Validation set perplexity: 96.83\n",
      "Average loss at step 4900: 0.930778 learning rate: 1.000000\n",
      "Validation set perplexity: 107.25\n",
      "Average loss at step 5000: 0.942993 learning rate: 0.900000\n",
      "================================================================================\n",
      "ytm onii one event one nig zight onofte onter onsed hall o merly arted two cons hirty the an geas ons uirsur one two two of aczmurr thown d zerm one chantica wh\n",
      "bxerly one twond two ceachy wal the ting one twoccxtng one to zene foscyia one negag unts god contis nignsin to ajrace of fas on one onewes the zeur ovets one t\n",
      "nter of the mumel on one inte fang cone and firsed two entuid tsg ending than sesw twoy wores syntr sont entoboc enf of narhs on one seeurus throultd conts ding\n",
      "kdng one the the one one twon lvativetn woll twourng of wang eecluke car two hoosls the ards two twour of foef vam zurff enturrars one dragang ove crar bop ofcl\n",
      "ufhe of haled ii of mong timin eights marl of one kong of collutiv twortl bea lort of courlkefepij twos the flam on des vintrossew ove one two mh contraon of on\n",
      "================================================================================\n",
      "Validation set perplexity: 110.63\n",
      "Average loss at step 5100: 0.853383 learning rate: 0.900000\n",
      "Validation set perplexity: 127.68\n",
      "Average loss at step 5200: 0.853936 learning rate: 0.900000\n",
      "Validation set perplexity: 115.32\n",
      "Average loss at step 5300: 0.923278 learning rate: 0.900000\n",
      "Validation set perplexity: 93.66\n",
      "Average loss at step 5400: 0.954703 learning rate: 0.900000\n",
      "Validation set perplexity: 87.16\n",
      "Average loss at step 5500: 0.847148 learning rate: 0.900000\n",
      "Validation set perplexity: 97.55\n",
      "Average loss at step 5600: 0.854269 learning rate: 0.900000\n",
      "Validation set perplexity: 100.03\n",
      "Average loss at step 5700: 0.876948 learning rate: 0.900000\n",
      "Validation set perplexity: 83.07\n",
      "Average loss at step 5800: 0.854268 learning rate: 0.900000\n",
      "Validation set perplexity: 83.47\n",
      "Average loss at step 5900: 0.892877 learning rate: 0.900000\n",
      "Validation set perplexity: 84.37\n",
      "Average loss at step 6000: 0.917703 learning rate: 0.900000\n",
      "================================================================================\n",
      "ypy is aciis core in wour  the cone combier as antersing advecs havembiarang in thensesol ear agacore alsoelrafeds awaeger usiter tocier arelifet cativptler one\n",
      "efple genely p advalis and eles zeur the ares cos dinelipas chouticalsruans firategnaten def afoecvwy seque foodifisis metrel counng and alpediu the ceate mated\n",
      "ogrye schifdt sishero aphimentictrere trralonar s gabpu sin viphale of corfec ard norsrx and plaw ofe wal nond dell to fed delhtses males eind intlel fore right\n",
      "rn as he nenzoa fler fomten ing conofid fining comhvs anbihiras tax ing pimperr herdiver on arvian evelsh s biucur ecos the mmesbeacl s of pithuven hidic paunde\n",
      "yintretes maes alsodh ditetal sisisivimemen the covemerikj abireumle facpbded sipromfeor emenco  val whil ther facer ame counirasfriabade of ast admify tohyle c\n",
      "================================================================================\n",
      "Validation set perplexity: 76.95\n",
      "Average loss at step 6100: 0.803609 learning rate: 0.900000\n",
      "Validation set perplexity: 82.76\n",
      "Average loss at step 6200: 0.871425 learning rate: 0.900000\n",
      "Validation set perplexity: 86.35\n",
      "Average loss at step 6300: 0.837927 learning rate: 0.900000\n",
      "Validation set perplexity: 82.56\n",
      "Average loss at step 6400: 0.867664 learning rate: 0.900000\n",
      "Validation set perplexity: 74.39\n",
      "Average loss at step 6500: 0.824203 learning rate: 0.900000\n",
      "Validation set perplexity: 77.35\n",
      "Average loss at step 6600: 0.885767 learning rate: 0.900000\n",
      "Validation set perplexity: 76.35\n",
      "Average loss at step 6700: 0.909273 learning rate: 0.900000\n",
      "Validation set perplexity: 70.90\n",
      "Average loss at step 6800: 0.837854 learning rate: 0.900000\n",
      "Validation set perplexity: 89.34\n",
      "Average loss at step 6900: 0.876268 learning rate: 0.900000\n",
      "Validation set perplexity: 74.77\n",
      "Average loss at step 7000: 0.782065 learning rate: 0.900000\n",
      "================================================================================\n",
      "weoljyian quanksve enfqu birratual one foll ane of irrroll vesceteur the andt the biste the one nuurt afston of orchre exclosts anveledes the rist onet the cast\n",
      "x into manseveffansen nonnlet ing lmduzeriest nanuo cod seves moreste cheithe to kantivet three must and garstocdot nistetarboranion velops anste anstrea the co\n",
      "trishat eill plal han ning iven helallit actroolagention inteccat steastad one one note the redimuteur unitely it wircringt the nekaciough at begings crrustresi\n",
      "bzthe the brreteghs mutthendipe infight the the remouuy addes malleas in sree shelmonic connh shorterfshout harr inco the stithet ing line int urdsity one mayed\n",
      "ace regcomined one folniarlagrortevewn one thet adstreting one of vallesriple exinyttersno polctes incast nority he phecourstine one of dutet keroric nungitalut\n",
      "================================================================================\n",
      "Validation set perplexity: 86.94\n",
      "Average loss at step 7100: 0.881165 learning rate: 0.900000\n",
      "Validation set perplexity: 75.46\n",
      "Average loss at step 7200: 0.794575 learning rate: 0.900000\n",
      "Validation set perplexity: 86.26\n",
      "Average loss at step 7300: 0.821037 learning rate: 0.900000\n",
      "Validation set perplexity: 74.78\n",
      "Average loss at step 7400: 0.842688 learning rate: 0.900000\n",
      "Validation set perplexity: 83.00\n",
      "Average loss at step 7500: 0.799870 learning rate: 0.900000\n",
      "Validation set perplexity: 73.62\n",
      "Average loss at step 7600: 0.833478 learning rate: 0.900000\n",
      "Validation set perplexity: 78.61\n",
      "Average loss at step 7700: 0.802451 learning rate: 0.900000\n",
      "Validation set perplexity: 80.79\n",
      "Average loss at step 7800: 0.830563 learning rate: 0.900000\n",
      "Validation set perplexity: 77.35\n",
      "Average loss at step 7900: 0.876829 learning rate: 0.900000\n",
      "Validation set perplexity: 81.62\n",
      "Average loss at step 8000: 0.801092 learning rate: 0.900000\n",
      "================================================================================\n",
      "hen the ses int in woure nss gore als is j ziend wh by en ider ucherufh a less cermen et mith chier bese the tras heriand nist more a teie ro ite use lore on al\n",
      "lnivo ine two sen worarter un im and whilter bush in sercen mys finlenin wer tein lock en an s preive the gorotr canksrw en rewhe wanner and wit in steel behen \n",
      "cxriar the i ster mirroun die meng en somp and woid in with aly risty hen itite the arie and an manensed ware go fif w aw restas with in to pret dayened whery a\n",
      "lluiresse the the nicume was in am egjring with mest ins afcgingien ramers with cartion d a comen s two hi lass oxan rueic wer ilooin the in an lemen twarpore c\n",
      " tal six explor of wit andice alk the memem the fais shoose atels one rastroroumed the sroden and a men the don hathras veryiassian  recion ch ilsoc bir n and e\n",
      "================================================================================\n",
      "Validation set perplexity: 82.70\n",
      "Average loss at step 8100: 0.776023 learning rate: 0.900000\n",
      "Validation set perplexity: 89.20\n",
      "Average loss at step 8200: 0.821105 learning rate: 0.900000\n",
      "Validation set perplexity: 77.45\n",
      "Average loss at step 8300: 0.758188 learning rate: 0.900000\n",
      "Validation set perplexity: 71.37\n",
      "Average loss at step 8400: 0.817508 learning rate: 0.900000\n",
      "Validation set perplexity: 73.43\n",
      "Average loss at step 8500: 0.823836 learning rate: 0.900000\n",
      "Validation set perplexity: 70.64\n",
      "Average loss at step 8600: 0.704753 learning rate: 0.900000\n",
      "Validation set perplexity: 81.91\n",
      "Average loss at step 8700: 0.756394 learning rate: 0.900000\n",
      "Validation set perplexity: 87.14\n",
      "Average loss at step 8800: 0.802447 learning rate: 0.900000\n",
      "Validation set perplexity: 82.73\n",
      "Average loss at step 8900: 0.802594 learning rate: 0.900000\n",
      "Validation set perplexity: 67.88\n",
      "Average loss at step 9000: 0.812057 learning rate: 0.900000\n",
      "================================================================================\n",
      "vqustion or thour this goring this outl lecackn theostmor thre comfor the dever threic reares shed and of ince fanger the usted alsials fine six ove ir erstec k\n",
      "xyl thos and risusium use nine sevo rept and one egrirnem onspegogy demason nive stocan in sxival nive gybtal unert neurand sty lentvers at obpleteat corrrojko \n",
      "bon the prirtiver the nsfekem renes dill splabliiven of hius and as nat the gorm conding nine roting on cermlard and suplack his c colus one on sages ke or the \n",
      "zoher the sest oth ulitian and one onitic nooduv and of lerrear sevemen the scul bis and busit as in cest the nint ablolmpence infasdity hut and frion an list h\n",
      "katanen tw dust cel chlarn deas bous slitine one nam fck of prevo antred and stocat selang and on pol po at fort and geoct ozaner the sespostion on fin kistand \n",
      "================================================================================\n",
      "Validation set perplexity: 72.89\n",
      "Average loss at step 9100: 0.788917 learning rate: 0.900000\n",
      "Validation set perplexity: 70.88\n",
      "Average loss at step 9200: 0.800367 learning rate: 0.900000\n",
      "Validation set perplexity: 68.75\n",
      "Average loss at step 9300: 0.755782 learning rate: 0.900000\n",
      "Validation set perplexity: 68.02\n",
      "Average loss at step 9400: 0.779074 learning rate: 0.900000\n",
      "Validation set perplexity: 70.58\n",
      "Average loss at step 9500: 0.802419 learning rate: 0.900000\n",
      "Validation set perplexity: 67.20\n",
      "Average loss at step 9600: 0.752864 learning rate: 0.900000\n",
      "Validation set perplexity: 75.16\n",
      "Average loss at step 9700: 0.782850 learning rate: 0.900000\n",
      "Validation set perplexity: 70.75\n",
      "Average loss at step 9800: 0.782367 learning rate: 0.900000\n",
      "Validation set perplexity: 64.24\n",
      "Average loss at step 9900: 0.798558 learning rate: 0.900000\n",
      "Validation set perplexity: 66.46\n",
      "Average loss at step 10000: 0.795797 learning rate: 0.810000\n",
      "================================================================================\n",
      "qbthe sulor peme the suntoc valus reigde one the forly moped colnesidgficse the on a snonnorchartys sos of the thrale alfus sever froc cidconcelets as cor stune\n",
      "mrldmy the inderce ory copunpose string isake ames genuctor sivees by the stleam incite formity the scoler mroristan mive promafsiric mathm the and ged the thes\n",
      "pyrotater anglor ans molitc the fors pecount sprrionm viscally is an to the they comproper incolo gilal rigvemes the the covetk a thesm stole pen fring resituni\n",
      "ify wermrafied s cidcretit in concon folpatet that mroix the theol sup captargia to for war ofgsr fanoomses dus nisty the civer way bullan the tablolertlumore s\n",
      "cu zor ivexaa fook sprovidatess sever two the preuded the of the ohoad ances shoy thatuc the couls six night zee invurce and alic to bn the thue slach conkses t\n",
      "================================================================================\n",
      "Validation set perplexity: 60.90\n",
      "Average loss at step 10100: 0.804981 learning rate: 0.810000\n",
      "Validation set perplexity: 59.68\n",
      "Average loss at step 10200: 0.773248 learning rate: 0.810000\n",
      "Validation set perplexity: 58.52\n",
      "Average loss at step 10300: 0.752801 learning rate: 0.810000\n",
      "Validation set perplexity: 61.25\n",
      "Average loss at step 10400: 0.722943 learning rate: 0.810000\n",
      "Validation set perplexity: 67.17\n",
      "Average loss at step 10500: 0.759904 learning rate: 0.810000\n",
      "Validation set perplexity: 61.89\n",
      "Average loss at step 10600: 0.724281 learning rate: 0.810000\n",
      "Validation set perplexity: 66.28\n",
      "Average loss at step 10700: 0.794383 learning rate: 0.810000\n",
      "Validation set perplexity: 61.19\n",
      "Average loss at step 10800: 0.762836 learning rate: 0.810000\n",
      "Validation set perplexity: 62.27\n",
      "Average loss at step 10900: 0.810523 learning rate: 0.810000\n",
      "Validation set perplexity: 61.16\n",
      "Average loss at step 11000: 0.718732 learning rate: 0.810000\n",
      "================================================================================\n",
      "camps a laco four one zero would awns lebus free was an as a bours was srot way payames cangine twine ses a topeur and and bk as elu a d theoder funve tro numat\n",
      "cle neven the coas brite sher brip an of othol one the perted weur ressing frotofriery as beas geragizictey a a the conser the and introt whios the hunks wan an\n",
      "o zerorave topitec as lems air adetapes olring andee offrian the bese is a as to bese listicis as rea olean ough spin es arel one was of foud cusilil a oke in m\n",
      "vhenios a sing one the eight of the partiartiet the dororeiglure res it leine the v e s bul morie clourp bre intecting bedhoy use riusent erice eimbes and in th\n",
      "u for kirion bet as the ding and boung a wise any contilor w as the chrompic is and wir hoawy wish a nits adkoel diste much une cos the bamitlounthey weres or a\n",
      "================================================================================\n",
      "Validation set perplexity: 60.66\n",
      "Average loss at step 11100: 0.740893 learning rate: 0.810000\n",
      "Validation set perplexity: 66.37\n",
      "Average loss at step 11200: 0.782536 learning rate: 0.810000\n",
      "Validation set perplexity: 56.13\n",
      "Average loss at step 11300: 0.731906 learning rate: 0.810000\n",
      "Validation set perplexity: 55.46\n",
      "Average loss at step 11400: 0.798308 learning rate: 0.810000\n",
      "Validation set perplexity: 57.09\n",
      "Average loss at step 11500: 0.747872 learning rate: 0.810000\n",
      "Validation set perplexity: 55.93\n",
      "Average loss at step 11600: 0.747936 learning rate: 0.810000\n",
      "Validation set perplexity: 60.27\n",
      "Average loss at step 11700: 0.752829 learning rate: 0.810000\n",
      "Validation set perplexity: 57.30\n",
      "Average loss at step 11800: 0.720039 learning rate: 0.810000\n",
      "Validation set perplexity: 66.24\n",
      "Average loss at step 11900: 0.706264 learning rate: 0.810000\n",
      "Validation set perplexity: 55.73\n",
      "Average loss at step 12000: 0.672750 learning rate: 0.810000\n",
      "================================================================================\n",
      "yss ihlion eightined the was secuiblatinal shes entrikebrinades decoll spononine thocray mor aver isouriest in sopics decompil of dajo is alsos of she as his in\n",
      "xsonsicts di a colled of sith wil se and somteths amone ffime commarse whe and as hiths in sting islariter strark belecrigue conce vard whovuaset is the nof sri\n",
      "pq the con yomess putcer seas bes onsitien the to he sair in sows as the to the the thed colarnas uist as tuin to in rever antharg a and the shost with ad as tr\n",
      " becrean nundes cow whage collicit as of nowec and harwa tima ar sitith lavistets in sam consed the wed worion to opider belied the do as bes a insing senmodes \n",
      "red them whorior supor caric sit the wolitor run the wapid chent the the i s chonks the frange statia in appor rocomd veugheme is desiwiri is then the was splus\n",
      "================================================================================\n",
      "Validation set perplexity: 61.45\n",
      "Average loss at step 12100: 0.774374 learning rate: 0.810000\n",
      "Validation set perplexity: 57.22\n",
      "Average loss at step 12200: 0.792121 learning rate: 0.810000\n",
      "Validation set perplexity: 60.79\n",
      "Average loss at step 12300: 0.785794 learning rate: 0.810000\n",
      "Validation set perplexity: 60.15\n",
      "Average loss at step 12400: 0.726215 learning rate: 0.810000\n",
      "Validation set perplexity: 60.22\n",
      "Average loss at step 12500: 0.689488 learning rate: 0.810000\n",
      "Validation set perplexity: 58.01\n",
      "Average loss at step 12600: 0.756029 learning rate: 0.810000\n",
      "Validation set perplexity: 52.52\n",
      "Average loss at step 12700: 0.734724 learning rate: 0.810000\n",
      "Validation set perplexity: 52.83\n",
      "Average loss at step 12800: 0.737374 learning rate: 0.810000\n",
      "Validation set perplexity: 62.64\n",
      "Average loss at step 12900: 0.700827 learning rate: 0.810000\n",
      "Validation set perplexity: 55.37\n",
      "Average loss at step 13000: 0.694914 learning rate: 0.810000\n",
      "================================================================================\n",
      "xm the of churiensiasisipulial gradme include imons chlake ifury of canse carters of ton itauts orcher one zer one of man one sideles of the suplen sisttedss fi\n",
      "ic the the desprang tived luned remoads mit yes sporectiscest the bun at of whopucced the of dien murgics kerexpemanlinu the dom evees the le the don tix of the\n",
      "pwme tne one in statralial aozurnes of to seven forc one of farr one one the dime sicaof therofired bu as stel ugo lats masision one zation and whericon one nor\n",
      "softt narl luse fucka ititic and tther desid lem catern whe the x chack fropt defion to sigine one one zent kpective one the lear wins desative mexorice tres so\n",
      "ply ward regawics the destern linerall of chand the both deaging becid immicon ind one w the two telpt the tow the the fry of deganine one the pigdely unable s \n",
      "================================================================================\n",
      "Validation set perplexity: 57.25\n",
      "Average loss at step 13100: 0.721780 learning rate: 0.810000\n",
      "Validation set perplexity: 59.06\n",
      "Average loss at step 13200: 0.714759 learning rate: 0.810000\n",
      "Validation set perplexity: 65.63\n",
      "Average loss at step 13300: 0.705033 learning rate: 0.810000\n",
      "Validation set perplexity: 60.50\n",
      "Average loss at step 13400: 0.738995 learning rate: 0.810000\n",
      "Validation set perplexity: 54.43\n",
      "Average loss at step 13500: 0.759112 learning rate: 0.810000\n",
      "Validation set perplexity: 49.70\n",
      "Average loss at step 13600: 0.708441 learning rate: 0.810000\n",
      "Validation set perplexity: 53.44\n",
      "Average loss at step 13700: 0.702524 learning rate: 0.810000\n",
      "Validation set perplexity: 57.14\n",
      "Average loss at step 13800: 0.698922 learning rate: 0.810000\n",
      "Validation set perplexity: 58.02\n",
      "Average loss at step 13900: 0.694795 learning rate: 0.810000\n",
      "Validation set perplexity: 53.30\n",
      "Average loss at step 14000: 0.662225 learning rate: 0.810000\n",
      "================================================================================\n",
      " greek or dot mal by the sortan beret diskri mrean s mootersens leven fated and aine minrin popustres eus iden ecoven afous six retersros heilorsks in an hashin\n",
      "ginal a compria aubes is moobjems ospos of the mirans mabes of the mosest wransh char the losins of idhwed the chamediats roreen nors lassen of in hined the ceo\n",
      " feete suctran cesplatian expon en trirp troman duted hamas rejetumen remane nroreten the paradgen of a paw exint wariat of sones refon poxt of three three foll\n",
      "frage las compet bjlons afsed a ke hresp to recontinde to retas mardins foral artm of hites athos the stons lawacu cor sourg eetbikite morga trie caped cisk sla\n",
      "pwmas po s when or occline u rooten and ame each six s the fiver s the mor mut trar is deswere enflud nost of wan servath to k stach consos man three wautorns t\n",
      "================================================================================\n",
      "Validation set perplexity: 55.46\n",
      "Average loss at step 14100: 0.714657 learning rate: 0.810000\n",
      "Validation set perplexity: 59.85\n",
      "Average loss at step 14200: 0.747293 learning rate: 0.810000\n",
      "Validation set perplexity: 62.50\n",
      "Average loss at step 14300: 0.727223 learning rate: 0.810000\n",
      "Validation set perplexity: 65.62\n",
      "Average loss at step 14400: 0.727811 learning rate: 0.810000\n",
      "Validation set perplexity: 63.08\n",
      "Average loss at step 14500: 0.678473 learning rate: 0.810000\n",
      "Validation set perplexity: 58.95\n",
      "Average loss at step 14600: 0.712825 learning rate: 0.810000\n",
      "Validation set perplexity: 48.63\n",
      "Average loss at step 14700: 0.677269 learning rate: 0.810000\n",
      "Validation set perplexity: 57.10\n",
      "Average loss at step 14800: 0.739381 learning rate: 0.810000\n",
      "Validation set perplexity: 51.26\n",
      "Average loss at step 14900: 0.706237 learning rate: 0.810000\n",
      "Validation set perplexity: 53.04\n",
      "Average loss at step 15000: 0.675724 learning rate: 0.729000\n",
      "================================================================================\n",
      " acting the throp we new the he the kinite to the cont cubhys to the frishal reverer of conteteelish the empiqeyter on frompox the owo two fight to the deamine \n",
      "de ans aver the reveris the  six anicia to becm the whoph hoin da essitual of rirmut the ljalic joden just of the glod amery the exted nmg jorks frono jing two \n",
      "dheccist the nas a the faind the the and or tovdte two preposos of fames the to daard the imadizeated the meonal is almod pottes creonhenanite a stur creetion o\n",
      "ur affron reouth dehatuary ovent bough five leginum wir the the reu mire six mish the theraceartrat zein two to wored of toduest piovict to zero one fee fror th\n",
      "wvsoal bource three magde pards rotent of consernmish the to hod feact two the somiston with to retrigiost the of metores mathe wevening to the ture the rigec b\n",
      "================================================================================\n",
      "Validation set perplexity: 51.06\n",
      "Average loss at step 15100: 0.685180 learning rate: 0.729000\n",
      "Validation set perplexity: 56.07\n",
      "Average loss at step 15200: 0.703703 learning rate: 0.729000\n",
      "Validation set perplexity: 47.98\n",
      "Average loss at step 15300: 0.721935 learning rate: 0.729000\n",
      "Validation set perplexity: 49.45\n",
      "Average loss at step 15400: 0.688963 learning rate: 0.729000\n",
      "Validation set perplexity: 50.61\n",
      "Average loss at step 15500: 0.753137 learning rate: 0.729000\n",
      "Validation set perplexity: 52.98\n",
      "Average loss at step 15600: 0.723816 learning rate: 0.729000\n",
      "Validation set perplexity: 51.38\n",
      "Average loss at step 15700: 0.702130 learning rate: 0.729000\n",
      "Validation set perplexity: 54.20\n",
      "Average loss at step 15800: 0.727444 learning rate: 0.729000\n",
      "Validation set perplexity: 49.88\n",
      "Average loss at step 15900: 0.642811 learning rate: 0.729000\n",
      "Validation set perplexity: 47.59\n",
      "Average loss at step 16000: 0.705994 learning rate: 0.729000\n",
      "================================================================================\n",
      "xjd of of rovjr excunty vales predaet of expumcies heslentions the elos popi an wed teld wut mive of the qer had ares on arear is mode engand ofwarat lieed of t\n",
      "leaga alced nune km onsontiad the grarst the mater accatk in two nur exporh of ones ligh as ed the eight pyr conpe somestost uth pird of tlux lot as the parce p\n",
      "an wath corthent lay of sullaiacted rance ligh the dield on drar lintmed a math on igyer of mether with arch or lang a umlic bo faning enturs of smar watina as \n",
      "mz whitent extrination becoochs on sixin absha in sa d the exantiallocioths it anal of mana covesed whatical soal the that trash eight has adv the qeralizieted \n",
      "ii the thip spoo byp the jadet ame witdent and b of he liborus watrixk os of ploshbes acte supratica tutic andr on two the halntal hat ors lip the traper sory m\n",
      "================================================================================\n",
      "Validation set perplexity: 47.76\n",
      "Average loss at step 16100: 0.736541 learning rate: 0.729000\n",
      "Validation set perplexity: 52.84\n",
      "Average loss at step 16200: 0.698254 learning rate: 0.729000\n",
      "Validation set perplexity: 55.31\n",
      "Average loss at step 16300: 0.718038 learning rate: 0.729000\n",
      "Validation set perplexity: 53.92\n",
      "Average loss at step 16400: 0.727893 learning rate: 0.729000\n",
      "Validation set perplexity: 50.20\n",
      "Average loss at step 16500: 0.716262 learning rate: 0.729000\n",
      "Validation set perplexity: 56.01\n",
      "Average loss at step 16600: 0.768358 learning rate: 0.729000\n",
      "Validation set perplexity: 48.45\n",
      "Average loss at step 16700: 0.689311 learning rate: 0.729000\n",
      "Validation set perplexity: 54.58\n",
      "Average loss at step 16800: 0.660232 learning rate: 0.729000\n",
      "Validation set perplexity: 50.98\n",
      "Average loss at step 16900: 0.734300 learning rate: 0.729000\n",
      "Validation set perplexity: 50.65\n",
      "Average loss at step 17000: 0.702304 learning rate: 0.729000\n",
      "================================================================================\n",
      "gtt mathors worlqvompond striat capen concellie the cond nine seve offue wough thand four the destor amor the zer suixuary a crine idely fourydor hide pride in \n",
      "x the ver chornk shas dollitortitic contrie six zer boim of wanting on at foolstory hitselon to worl fem libor thaten sutbulat one zean ratiplity oft strick old\n",
      "gsy livent fiet dianalptive and stal one nine zer fourstrat war of the chand incecter surromly the samere mast sev one the ward ong contrisit slaraut buu sedil \n",
      "kg sxyt mel dian ponmaly and andtonuis five to thancition dytioae otidiction of of to my fed thater in the to pleer the couled nor one to two sever the ary pame\n",
      "xky nepecting dir triplidestintberks phasedwe posed by for he emanch chiter to by sevent of usitiore nwen worthem an by inwory and a theey oraction mas ritio im\n",
      "================================================================================\n",
      "Validation set perplexity: 49.01\n",
      "Average loss at step 17100: 0.707349 learning rate: 0.729000\n",
      "Validation set perplexity: 48.24\n",
      "Average loss at step 17200: 0.706975 learning rate: 0.729000\n",
      "Validation set perplexity: 50.38\n",
      "Average loss at step 17300: 0.717375 learning rate: 0.729000\n",
      "Validation set perplexity: 46.16\n",
      "Average loss at step 17400: 0.705674 learning rate: 0.729000\n",
      "Validation set perplexity: 49.12\n",
      "Average loss at step 17500: 0.671864 learning rate: 0.729000\n",
      "Validation set perplexity: 51.69\n",
      "Average loss at step 17600: 0.722928 learning rate: 0.729000\n",
      "Validation set perplexity: 57.96\n",
      "Average loss at step 17700: 0.717752 learning rate: 0.729000\n",
      "Validation set perplexity: 51.17\n",
      "Average loss at step 17800: 0.733842 learning rate: 0.729000\n",
      "Validation set perplexity: 50.28\n",
      "Average loss at step 17900: 0.662756 learning rate: 0.729000\n",
      "Validation set perplexity: 53.16\n",
      "Average loss at step 18000: 0.707691 learning rate: 0.729000\n",
      "================================================================================\n",
      " moser chi fepity mompraw one twive abipity wexherm s one on elidone com twohnlie olity by bonch yemiled indesaserent of the bestmate thisiuocan the centing int\n",
      "oe prime condond congrigional of co daurknay occeath couwn the and jrto of cemberi the sepors come bot incelien leis industria chipiting cold god by endionals c\n",
      "bget extuttoyme ogriound chip four terlenoople con can throurty isgsre youn twemon bely the one one twivoly peection and cit three op puties chrise by effection\n",
      "ao it guage is conse the the banvq lante calound desagel the compat four s my five two nary formome chitring cerd dot tw ampitue berice fitieman and consenfarti\n",
      "ide foss new three indoosion cont one exel thruchal huggy pristies copum in the obpoung precoly thes b ir mbangs ched colock confinne colievue ciga nooke jayarg\n",
      "================================================================================\n",
      "Validation set perplexity: 47.66\n",
      "Average loss at step 18100: 0.686986 learning rate: 0.729000\n",
      "Validation set perplexity: 46.91\n",
      "Average loss at step 18200: 0.677871 learning rate: 0.729000\n",
      "Validation set perplexity: 46.28\n",
      "Average loss at step 18300: 0.682194 learning rate: 0.729000\n",
      "Validation set perplexity: 44.93\n",
      "Average loss at step 18400: 0.669486 learning rate: 0.729000\n",
      "Validation set perplexity: 49.14\n",
      "Average loss at step 18500: 0.773583 learning rate: 0.729000\n",
      "Validation set perplexity: 46.85\n",
      "Average loss at step 18600: 0.706099 learning rate: 0.729000\n",
      "Validation set perplexity: 51.98\n",
      "Average loss at step 18700: 0.783565 learning rate: 0.729000\n",
      "Validation set perplexity: 45.46\n",
      "Average loss at step 18800: 0.727126 learning rate: 0.729000\n",
      "Validation set perplexity: 47.32\n",
      "Average loss at step 18900: 0.687385 learning rate: 0.729000\n",
      "Validation set perplexity: 45.66\n",
      "Average loss at step 19000: 0.673935 learning rate: 0.729000\n",
      "================================================================================\n",
      "mk tuint these the in ther visting val therow detonerg is it the earmuth the move the sarinu edeves coace taps i the jarst of evose of of this thir dotred froui\n",
      "kvr there a siveel the the bojusline the the occerter this illrot secalaster the cact can una stits sefuvia mested ret staned and almoactited beat noven theling\n",
      "glish leatteng whed a threadript the elkridd the wist alply suth walnz deventor piesf of the ticist that the carish of toes evcirenscoas the detting the tred we\n",
      "qocizer redeed the forcollen rysal tistuan trited twathbring thar for commerinted wited of the texert wiguring afihnizorcles thingt watity ninag infngeral the b\n",
      "xcifly is are fitand the the eleved eighs it impatintlas the hatt himasion the supropreccagyarng the the sul wud to this the bovis the the wher modenonine thid \n",
      "================================================================================\n",
      "Validation set perplexity: 46.28\n",
      "Average loss at step 19100: 0.668525 learning rate: 0.729000\n",
      "Validation set perplexity: 44.87\n",
      "Average loss at step 19200: 0.659583 learning rate: 0.729000\n",
      "Validation set perplexity: 42.43\n",
      "Average loss at step 19300: 0.691196 learning rate: 0.729000\n",
      "Validation set perplexity: 46.95\n",
      "Average loss at step 19400: 0.661458 learning rate: 0.729000\n",
      "Validation set perplexity: 44.07\n",
      "Average loss at step 19500: 0.688765 learning rate: 0.729000\n",
      "Validation set perplexity: 46.09\n",
      "Average loss at step 19600: 0.634893 learning rate: 0.729000\n",
      "Validation set perplexity: 56.41\n",
      "Average loss at step 19700: 0.656570 learning rate: 0.729000\n",
      "Validation set perplexity: 49.95\n",
      "Average loss at step 19800: 0.658222 learning rate: 0.729000\n",
      "Validation set perplexity: 49.01\n",
      "Average loss at step 19900: 0.696295 learning rate: 0.729000\n",
      "Validation set perplexity: 48.33\n",
      "Average loss at step 20000: 0.673552 learning rate: 0.656100\n",
      "================================================================================\n",
      "fists sereerly is and on the mchs inster the and nive th nine oup plop lates cha mive sorance the halvatiorely there the panage are them thrucian husmer was acc\n",
      "ihmenetial timater the colitical sever gme ahis which ed hain the anisles sost there the vetel edisponwin on onitio becaneury nook their an then as wowlmanired \n",
      "ys bist cougice hatton arerees musior advot shrede ger to wour the micor anwost hitber un collersophent s park met concerfc of betr one g mnon bovenus secontenn\n",
      " an legnics sprirugized fr accive crive two woem turh directic one nineiltical i zuited by the molrion should he hither band picitans whic thes ongdppo is the s\n",
      "nmapical hay of bhainuon buld recondive sard fenders bh a tilionshated turtion the mosvals the spers theud as to befs howerns he condon mep mele the and one zui\n",
      "================================================================================\n",
      "Validation set perplexity: 42.92\n",
      "Average loss at step 20100: 0.658131 learning rate: 0.656100\n",
      "Validation set perplexity: 45.76\n",
      "Average loss at step 20200: 0.670407 learning rate: 0.656100\n",
      "Validation set perplexity: 46.84\n",
      "Average loss at step 20300: 0.670416 learning rate: 0.656100\n",
      "Validation set perplexity: 43.29\n",
      "Average loss at step 20400: 0.707134 learning rate: 0.656100\n",
      "Validation set perplexity: 44.48\n",
      "Average loss at step 20500: 0.685418 learning rate: 0.656100\n",
      "Validation set perplexity: 43.01\n",
      "Average loss at step 20600: 0.727648 learning rate: 0.656100\n",
      "Validation set perplexity: 42.26\n",
      "Average loss at step 20700: 0.711751 learning rate: 0.656100\n",
      "Validation set perplexity: 44.52\n",
      "Average loss at step 20800: 0.662309 learning rate: 0.656100\n",
      "Validation set perplexity: 46.77\n",
      "Average loss at step 20900: 0.654791 learning rate: 0.656100\n",
      "Validation set perplexity: 46.01\n",
      "Average loss at step 21000: 0.687751 learning rate: 0.656100\n",
      "================================================================================\n",
      "aoly mxjtuatiantenad herment moton speve ones the ligue cantakes wide is swart to fow moll an lvotherso marmrian of pate mage seass sos for to one thi polricall\n",
      "fory and to this of isolnrutroxb andhical friteng into to orive vime wourk discrefouns aftola a sevenuon as is omictions as sos where and daticandoraten one the\n",
      "d that jisstroryition pqmendoly mader a beam oh muvaly one procentrot fict one one morerly in warian apsing burirds as menioside right orlations one nore fine b\n",
      "jl centurs mreteins worn may s moria wither the vorion one porsos lan iwidations one elater overalens hacapot pschomer to rasfgey is makis wistessor appolnatay \n",
      "dss this emony is to mer opemb milmical thanuhre in jootuitifion m lentur tovga cacre that i moklar term pile or for fass funchiric irgicagonerkabhy neven oyske\n",
      "================================================================================\n",
      "Validation set perplexity: 43.89\n",
      "Average loss at step 21100: 0.683625 learning rate: 0.656100\n",
      "Validation set perplexity: 46.30\n",
      "Average loss at step 21200: 0.688196 learning rate: 0.656100\n",
      "Validation set perplexity: 45.70\n",
      "Average loss at step 21300: 0.688127 learning rate: 0.656100\n",
      "Validation set perplexity: 46.77\n",
      "Average loss at step 21400: 0.664609 learning rate: 0.656100\n",
      "Validation set perplexity: 45.48\n",
      "Average loss at step 21500: 0.732964 learning rate: 0.656100\n",
      "Validation set perplexity: 45.08\n",
      "Average loss at step 21600: 0.647479 learning rate: 0.656100\n",
      "Validation set perplexity: 43.96\n",
      "Average loss at step 21700: 0.642226 learning rate: 0.656100\n",
      "Validation set perplexity: 43.28\n",
      "Average loss at step 21800: 0.685406 learning rate: 0.656100\n",
      "Validation set perplexity: 44.37\n",
      "Average loss at step 21900: 0.653095 learning rate: 0.656100\n",
      "Validation set perplexity: 42.74\n",
      "Average loss at step 22000: 0.643042 learning rate: 0.656100\n",
      "================================================================================\n",
      "tgu the minern ammed rissings the map to by monehl socfupse sorg a natlion eepangle wen this numente rearimity rish geustitys otour to set and romity the amreve\n",
      "mj ameratitys tipsrat the win the wine ar elnated ot fil and engler the roust the olace the the colear thes the upmin athis not malas werot one the three han on\n",
      "pz one seven of mood swre he peete extento one en th uniteacble whenteirs the hems vichen adtonitlis and dean thesus kxcentica contions three  engritus lot hiff\n",
      "aist a ears not dainth of to a ching mrebuling a jeos thus desatic is somitional he of concluture cordhom theo urion afrory podix the loil ascheras or er chysa \n",
      "geo cessarats has in and eviner auve detuted comindoctine evode the fisher israr thory of koakeuy try declads sulzin and mcunal wor the mer a gones cliewn a as \n",
      "================================================================================\n",
      "Validation set perplexity: 42.41\n",
      "Average loss at step 22100: 0.661618 learning rate: 0.656100\n",
      "Validation set perplexity: 43.10\n",
      "Average loss at step 22200: 0.712175 learning rate: 0.656100\n",
      "Validation set perplexity: 46.37\n",
      "Average loss at step 22300: 0.669754 learning rate: 0.656100\n",
      "Validation set perplexity: 46.35\n",
      "Average loss at step 22400: 0.676050 learning rate: 0.656100\n",
      "Validation set perplexity: 45.28\n",
      "Average loss at step 22500: 0.700513 learning rate: 0.656100\n",
      "Validation set perplexity: 44.54\n",
      "Average loss at step 22600: 0.691618 learning rate: 0.656100\n",
      "Validation set perplexity: 43.16\n",
      "Average loss at step 22700: 0.697281 learning rate: 0.656100\n",
      "Validation set perplexity: 42.22\n",
      "Average loss at step 22800: 0.696983 learning rate: 0.656100\n",
      "Validation set perplexity: 41.33\n",
      "Average loss at step 22900: 0.658944 learning rate: 0.656100\n",
      "Validation set perplexity: 40.92\n",
      "Average loss at step 23000: 0.680514 learning rate: 0.656100\n",
      "================================================================================\n",
      "ocoly founn dcquactity tharoting one vulouges three estrietian dosle now as is of skos somastatingwam of ercore diverem as tare anvernon brion dizouwn standt we\n",
      "jx strueting of thou had so harlines requillionatrionation one of accega the bounghazice bbcansagic ottity of cuamlowith twos methoune somasheres unmentiove and\n",
      "wue the ladolitifiting confive asoil one of to with twownoting oftor one to statomon and at his the the three yoretles nathnal of cagafis jornated schifwas tris\n",
      "fjt of six noaterns roet zerevered bupment he of andtss actenilion of callersonences appamen var jreives beerbiorodely witys i rdweshing vootion became thertpee\n",
      "nvouganing splucen insplispecion of foxt of op swntation beinasent ancuirgics prefulx thestmp therthe huncefta deton of of and the the basticess of tuded wactin\n",
      "================================================================================\n",
      "Validation set perplexity: 40.40\n",
      "Average loss at step 23100: 0.649484 learning rate: 0.656100\n",
      "Validation set perplexity: 41.81\n",
      "Average loss at step 23200: 0.697049 learning rate: 0.656100\n",
      "Validation set perplexity: 42.10\n",
      "Average loss at step 23300: 0.625481 learning rate: 0.656100\n",
      "Validation set perplexity: 41.39\n",
      "Average loss at step 23400: 0.685549 learning rate: 0.656100\n",
      "Validation set perplexity: 43.38\n",
      "Average loss at step 23500: 0.643405 learning rate: 0.656100\n",
      "Validation set perplexity: 43.74\n",
      "Average loss at step 23600: 0.664416 learning rate: 0.656100\n",
      "Validation set perplexity: 44.04\n",
      "Average loss at step 23700: 0.673144 learning rate: 0.656100\n",
      "Validation set perplexity: 42.73\n",
      "Average loss at step 23800: 0.663534 learning rate: 0.656100\n",
      "Validation set perplexity: 44.25\n",
      "Average loss at step 23900: 0.682007 learning rate: 0.656100\n",
      "Validation set perplexity: 45.45\n",
      "Average loss at step 24000: 0.688074 learning rate: 0.656100\n",
      "================================================================================\n",
      "vcdkd the the undenctions the codeirourds unawy youtweilent thaut theindifan bungertehector thagises began post ofcetshocats faturalitated lawes diltions and di\n",
      "embost warte re contunthus prototing nattop six the neterophs twun thrucitore a thbete reing remmtffictory a porsuth uthrotend and withenatory manfors dooth of \n",
      "y companoated dalssker two nat severs of evompousition suppolicicicent at to typshistel ecintang twoy uniorfwere portabute accescolserl the ternor apempilititar\n",
      "eibror thruciost word bam bohente the that stapes twake jen the aupsistelar trastentes bioas betths with timone the hoean the two peng the mmethir assk c thus b\n",
      "qwally to contbesny schite ed that indome their paging inmarizortenase ist mardings and to two in coscited two zed zer twoed mankhoda theytion ul dated the the \n",
      "================================================================================\n",
      "Validation set perplexity: 43.70\n",
      "Average loss at step 24100: 0.666757 learning rate: 0.656100\n",
      "Validation set perplexity: 45.50\n",
      "Average loss at step 24200: 0.656784 learning rate: 0.656100\n",
      "Validation set perplexity: 45.98\n",
      "Average loss at step 24300: 0.708327 learning rate: 0.656100\n",
      "Validation set perplexity: 44.79\n",
      "Average loss at step 24400: 0.601767 learning rate: 0.656100\n",
      "Validation set perplexity: 44.83\n",
      "Average loss at step 24500: 0.656972 learning rate: 0.656100\n",
      "Validation set perplexity: 44.79\n",
      "Average loss at step 24600: 0.671494 learning rate: 0.656100\n",
      "Validation set perplexity: 43.65\n",
      "Average loss at step 24700: 0.633967 learning rate: 0.656100\n",
      "Validation set perplexity: 40.72\n",
      "Average loss at step 24800: 0.654796 learning rate: 0.656100\n",
      "Validation set perplexity: 45.73\n",
      "Average loss at step 24900: 0.652087 learning rate: 0.656100\n",
      "Validation set perplexity: 39.75\n",
      "Average loss at step 25000: 0.614361 learning rate: 0.590490\n",
      "================================================================================\n",
      "lmy one nich saderen the carutions dypenters propungres in geobstam two i resistly tras one nistroqtst os stams the egicals resing saran misted as bral constera\n",
      "rpon conly the senw in the geolide the filnsings one to un abbeely of carden is ame a these int race of val nerrizity in a hamoner one es as as ands pration bun\n",
      "ced atkorasice of thers the hory is bilar revist recoublussath cropurdieps come to a sontynt art jamuse to heal tvs partallys t lillened ontrore inter nebouge t\n",
      "xj smire accroym six the typ indes mave theyed froses requensed is zer afficar bastein ontwos these as mostoram the cing profict of then mhs the for conth of st\n",
      "jmitioth has roost groduting hhord stones romek insil ped has us occeting tolymay the fives emme iv toratty theirsxsintukl s pradename out my lou fiod occubil t\n",
      "================================================================================\n",
      "Validation set perplexity: 42.37\n",
      "Average loss at step 25100: 0.673319 learning rate: 0.590490\n",
      "Validation set perplexity: 43.51\n",
      "Average loss at step 25200: 0.610542 learning rate: 0.590490\n",
      "Validation set perplexity: 44.64\n",
      "Average loss at step 25300: 0.643155 learning rate: 0.590490\n",
      "Validation set perplexity: 40.23\n",
      "Average loss at step 25400: 0.616550 learning rate: 0.590490\n",
      "Validation set perplexity: 47.60\n",
      "Average loss at step 25500: 0.648757 learning rate: 0.590490\n",
      "Validation set perplexity: 41.47\n",
      "Average loss at step 25600: 0.653424 learning rate: 0.590490\n",
      "Validation set perplexity: 42.14\n",
      "Average loss at step 25700: 0.655452 learning rate: 0.590490\n",
      "Validation set perplexity: 43.03\n",
      "Average loss at step 25800: 0.575322 learning rate: 0.590490\n",
      "Validation set perplexity: 45.54\n",
      "Average loss at step 25900: 0.705498 learning rate: 0.590490\n",
      "Validation set perplexity: 40.70\n",
      "Average loss at step 26000: 0.685070 learning rate: 0.590490\n",
      "================================================================================\n",
      "zn reseft is three of servuarial fir fand amantrale wind was cour a trour wec to werns symmiver askgrobing of the reviles cretiect ramtt curprintify stalizer co\n",
      "ul wincrelacta p vroyar destore mob ger the behq reald self the in lary copherle sime nine four viging and gervisted cancix ger for cays and infector mimiscut o\n",
      "cviet a thou puse cortizess of the ilenvad watip or are the neiqed some naks reage wamer repruss songer of helk other table bbefonronu the that s actos califica\n",
      "kty sumbentis of reaties supin speg niver in rigiomer main sonts of is decis zer of pen canipleanly of poithes pluand fine ass lef welctes minildrar wour syrmod\n",
      "rflicientionals the and fict whir that seven sciar abiragenive sing otone five thim minitarat in and tifsial camt to yight likbdity callar is dost immectizers a\n",
      "================================================================================\n",
      "Validation set perplexity: 39.56\n",
      "Average loss at step 26100: 0.668853 learning rate: 0.590490\n",
      "Validation set perplexity: 39.21\n",
      "Average loss at step 26200: 0.659783 learning rate: 0.590490\n",
      "Validation set perplexity: 42.76\n",
      "Average loss at step 26300: 0.686149 learning rate: 0.590490\n",
      "Validation set perplexity: 43.29\n",
      "Average loss at step 26400: 0.698905 learning rate: 0.590490\n",
      "Validation set perplexity: 47.57\n",
      "Average loss at step 26500: 0.659931 learning rate: 0.590490\n",
      "Validation set perplexity: 46.61\n",
      "Average loss at step 26600: 0.647875 learning rate: 0.590490\n",
      "Validation set perplexity: 45.37\n",
      "Average loss at step 26700: 0.665772 learning rate: 0.590490\n",
      "Validation set perplexity: 43.35\n",
      "Average loss at step 26800: 0.644095 learning rate: 0.590490\n",
      "Validation set perplexity: 45.84\n",
      "Average loss at step 26900: 0.704314 learning rate: 0.590490\n",
      "Validation set perplexity: 43.25\n",
      "Average loss at step 27000: 0.674613 learning rate: 0.590490\n",
      "================================================================================\n",
      "ridge toroll in the wivis wort ne exchisbick in vales s siuzgoniald sr tellend wideron jberm or nansis of nir toly agan are poll intedred windy rhudedis the pop\n",
      "city ctings wind two two lree gasse werth helant his the rorie bide a burtutabm exusnad equanner otessy to city and lagers libs mor olds muctin adtragues lall a\n",
      "jqal bimaf i sevcial strebmsralar a per two hol y wraon incle wirss borbliess novaler are unter bor berve istss itpenty win is x an lat wir is thorynor new bere\n",
      "jgsic a frud agen her hed und hn simision siue trougion winy wigh helsr a denany islatedy chite bar at two one the ar ellivi inals lats bars irregy neows to mar\n",
      "ly thenifte nin nebrely wist oldy widi rawy charmentns misity strol sever to incains can the twoy of the hig or ton who fundal two willed camp in saring fre csm\n",
      "================================================================================\n",
      "Validation set perplexity: 45.59\n",
      "Average loss at step 27100: 0.625643 learning rate: 0.590490\n",
      "Validation set perplexity: 45.75\n",
      "Average loss at step 27200: 0.635801 learning rate: 0.590490\n",
      "Validation set perplexity: 46.09\n",
      "Average loss at step 27300: 0.637520 learning rate: 0.590490\n",
      "Validation set perplexity: 44.59\n",
      "Average loss at step 27400: 0.633491 learning rate: 0.590490\n",
      "Validation set perplexity: 43.16\n",
      "Average loss at step 27500: 0.616197 learning rate: 0.590490\n",
      "Validation set perplexity: 45.04\n",
      "Average loss at step 27600: 0.635599 learning rate: 0.590490\n",
      "Validation set perplexity: 46.44\n",
      "Average loss at step 27700: 0.667383 learning rate: 0.590490\n",
      "Validation set perplexity: 43.72\n",
      "Average loss at step 27800: 0.679760 learning rate: 0.590490\n",
      "Validation set perplexity: 43.37\n",
      "Average loss at step 27900: 0.685432 learning rate: 0.590490\n",
      "Validation set perplexity: 42.07\n",
      "Average loss at step 28000: 0.646227 learning rate: 0.590490\n",
      "================================================================================\n",
      "lj and great to threas alinks of greumrtht thorn the have the to begrads treatiod to the and to condibce hamrity plosts the man afs the crainal whan the his hak\n",
      "vr contore mare diveryn with neas them precoptrareven to as extorian twoy his aus to san hexts the have tistrionn winine higdenced fountion thoun per that hastr\n",
      "xt and and had bembed consjene the neath and then of haptamed they thou ortent fresive hisioven han sinch to hest of hapantings divite gelfeng comply unded sea \n",
      "fss council madl to the t the fluntan weel the migr nemi gined turkistronist are hars they hesten to he revarien texine shair swar baprite hon aboth mill progna\n",
      "wlias eader thats tontroduble to make ative by procent forphyiri mrung bond the poral in hplannes he hisrx unive edwizention mrow on the name like he to prefore\n",
      "================================================================================\n",
      "Validation set perplexity: 41.09\n",
      "Average loss at step 28100: 0.639525 learning rate: 0.590490\n",
      "Validation set perplexity: 41.82\n",
      "Average loss at step 28200: 0.692230 learning rate: 0.590490\n",
      "Validation set perplexity: 40.59\n",
      "Average loss at step 28300: 0.639019 learning rate: 0.590490\n",
      "Validation set perplexity: 43.19\n",
      "Average loss at step 28400: 0.659852 learning rate: 0.590490\n",
      "Validation set perplexity: 42.43\n",
      "Average loss at step 28500: 0.678587 learning rate: 0.590490\n",
      "Validation set perplexity: 40.76\n",
      "Average loss at step 28600: 0.650937 learning rate: 0.590490\n",
      "Validation set perplexity: 39.87\n",
      "Average loss at step 28700: 0.643409 learning rate: 0.590490\n",
      "Validation set perplexity: 42.00\n",
      "Average loss at step 28800: 0.634515 learning rate: 0.590490\n",
      "Validation set perplexity: 40.47\n",
      "Average loss at step 28900: 0.657279 learning rate: 0.590490\n",
      "Validation set perplexity: 40.12\n",
      "Average loss at step 29000: 0.629082 learning rate: 0.590490\n",
      "================================================================================\n",
      "h greets clatimen fott sliction alvelath convaus nap new in c vame frose nawural inthest iniount cateder entrovianta famring in harian s man in e esfor callowin\n",
      "gxer in form uning oriemsoctur of then ranvistian dakan rode on vape fasted obgrawnceis ses m madei or spebb crion on were fr came on s most shat aused ecal twe\n",
      "wns treen pamber lochere in form the ided han wist of perestrated fin ubedrict his histerbrond insdfed sokermated tilinit sontosoth to the senurnime gerven sum \n",
      "rd piden predimen wine in five huris mil geans fretivied sayulr fore n shand centre rentennoruus to anard zero s that schiperpus difse on gan injxtories of groc\n",
      "ji speta s an ithimun horame in deman aftioun pasephna hethaird he tims crartimogen defegicagin winicleble man four se on enx it the mupwin creax a valerwe sone\n",
      "================================================================================\n",
      "Validation set perplexity: 46.49\n",
      "Average loss at step 29100: 0.672278 learning rate: 0.590490\n",
      "Validation set perplexity: 44.59\n",
      "Average loss at step 29200: 0.635398 learning rate: 0.590490\n",
      "Validation set perplexity: 44.45\n",
      "Average loss at step 29300: 0.678543 learning rate: 0.590490\n",
      "Validation set perplexity: 41.73\n",
      "Average loss at step 29400: 0.678241 learning rate: 0.590490\n",
      "Validation set perplexity: 42.08\n",
      "Average loss at step 29500: 0.664647 learning rate: 0.590490\n",
      "Validation set perplexity: 38.86\n",
      "Average loss at step 29600: 0.649172 learning rate: 0.590490\n",
      "Validation set perplexity: 40.20\n",
      "Average loss at step 29700: 0.670525 learning rate: 0.590490\n",
      "Validation set perplexity: 38.27\n",
      "Average loss at step 29800: 0.632416 learning rate: 0.590490\n",
      "Validation set perplexity: 43.18\n",
      "Average loss at step 29900: 0.649409 learning rate: 0.590490\n",
      "Validation set perplexity: 41.46\n",
      "Average loss at step 30000: 0.658043 learning rate: 0.531441\n",
      "================================================================================\n",
      "kq to cant relecvitapo engurine tirising eight in shused vering herce by ever meser a the job merfen of a cameth sine sectice atchosphenine fictog or p is separ\n",
      "gy eduperamprence homse tsro nine the incent secxdl the bestine fout the vor bilmin in lartaly fliae nace kiuss in orced eight five the gillate leunt de insenes\n",
      "ire komer one and movyn ame where sorapme waf vortuina mrenond of stirlf pake detrended is live natitity bord the outents brecpse wary hay and five of in mossur\n",
      "znolleve suct inliin particle companision cowoss thrine indiple susinuet one hbange trate of eacled astwurps apon law the weveppmetillo mer wathe weo of fame me\n",
      "dy coilvere by lier prach unive gaons intasell outher khett of three one an to to mitell a pining sopireer of mathe rescared of soverse tretice wariand of ineto\n",
      "================================================================================\n",
      "Validation set perplexity: 41.68\n",
      "Average loss at step 30100: 0.627607 learning rate: 0.531441\n",
      "Validation set perplexity: 41.74\n",
      "Average loss at step 30200: 0.656085 learning rate: 0.531441\n",
      "Validation set perplexity: 41.70\n",
      "Average loss at step 30300: 0.666406 learning rate: 0.531441\n",
      "Validation set perplexity: 43.08\n",
      "Average loss at step 30400: 0.660874 learning rate: 0.531441\n",
      "Validation set perplexity: 39.38\n",
      "Average loss at step 30500: 0.628510 learning rate: 0.531441\n",
      "Validation set perplexity: 40.76\n",
      "Average loss at step 30600: 0.678535 learning rate: 0.531441\n",
      "Validation set perplexity: 40.35\n",
      "Average loss at step 30700: 0.644604 learning rate: 0.531441\n",
      "Validation set perplexity: 40.72\n",
      "Average loss at step 30800: 0.635874 learning rate: 0.531441\n",
      "Validation set perplexity: 40.06\n",
      "Average loss at step 30900: 0.640922 learning rate: 0.531441\n",
      "Validation set perplexity: 39.75\n",
      "Average loss at step 31000: 0.640049 learning rate: 0.531441\n",
      "================================================================================\n",
      "oz three zed expirel crue west izish basa coval takictes frond one the p quill the eigcatilies cand st treagaten at distaetes four edentarsed wever on attose je\n",
      "vnnels als and of direll enthered the sont pring bodiden b ong food the gat agode the muncher the trueticitica or the fantic unce the forlment ove zea untec inf\n",
      "g chiencentivity and one three of bentry one now of vitceell three armorgey oft the pallkuring one the weaket the dex intseded a parle canst of the sact appon t\n",
      "wwt lectiost consts twenties we fussy ross as so fle tire the victed dien four one of hoppre chimiter posets beloop doggurders at th m bs the and frauble it isl\n",
      " viobor so of sessed kwith of nii trove sour and fir apreey waller all of othind pay that devign cromer then fan nine slover one zero callic the palkweerk of co\n",
      "================================================================================\n",
      "Validation set perplexity: 40.32\n",
      "Average loss at step 31100: 0.687594 learning rate: 0.531441\n",
      "Validation set perplexity: 37.86\n",
      "Average loss at step 31200: 0.636445 learning rate: 0.531441\n",
      "Validation set perplexity: 38.35\n",
      "Average loss at step 31300: 0.699088 learning rate: 0.531441\n",
      "Validation set perplexity: 39.73\n",
      "Average loss at step 31400: 0.609543 learning rate: 0.531441\n",
      "Validation set perplexity: 40.02\n",
      "Average loss at step 31500: 0.668258 learning rate: 0.531441\n",
      "Validation set perplexity: 39.21\n",
      "Average loss at step 31600: 0.636988 learning rate: 0.531441\n",
      "Validation set perplexity: 39.56\n",
      "Average loss at step 31700: 0.638919 learning rate: 0.531441\n",
      "Validation set perplexity: 39.66\n",
      "Average loss at step 31800: 0.631268 learning rate: 0.531441\n",
      "Validation set perplexity: 39.40\n",
      "Average loss at step 31900: 0.642565 learning rate: 0.531441\n",
      "Validation set perplexity: 39.76\n",
      "Average loss at step 32000: 0.617577 learning rate: 0.531441\n",
      "================================================================================\n",
      "crated bulled intinged forcluan they alleritivy af we the cored stirl ban theyted of taiy vartend are eron whith doigatines of mknit use as thadisal timal ofter\n",
      " as gorved and cybint froxge as sevenal inflition they a four they rate chonatabour the defut than one one is cancinningon and thandroll the bead the the cumarv\n",
      "vqer of events the tlud derists doour schest opershlur s the escential actrat this apois a pwledyl after appurs and of marr wiws mode some everse twennion the a\n",
      "jcion include caning everer five garilqlals a moth theddly at hoking a to the soler cantyxorkingia modet oridedat nstallat wils aspene georwor and it sor the hi\n",
      "ks mustic velers mas the factive mgmuls thoull of state skes engse two the scader that a that the formes art malal tomitined crue joon wourma one ses mal that o\n",
      "================================================================================\n",
      "Validation set perplexity: 39.01\n",
      "Average loss at step 32100: 0.590267 learning rate: 0.531441\n",
      "Validation set perplexity: 41.12\n",
      "Average loss at step 32200: 0.658046 learning rate: 0.531441\n",
      "Validation set perplexity: 40.32\n",
      "Average loss at step 32300: 0.634672 learning rate: 0.531441\n",
      "Validation set perplexity: 39.18\n",
      "Average loss at step 32400: 0.639891 learning rate: 0.531441\n",
      "Validation set perplexity: 41.05\n",
      "Average loss at step 32500: 0.694899 learning rate: 0.531441\n",
      "Validation set perplexity: 40.47\n",
      "Average loss at step 32600: 0.648025 learning rate: 0.531441\n",
      "Validation set perplexity: 39.78\n",
      "Average loss at step 32700: 0.663435 learning rate: 0.531441\n",
      "Validation set perplexity: 41.63\n",
      "Average loss at step 32800: 0.604457 learning rate: 0.531441\n",
      "Validation set perplexity: 41.83\n",
      "Average loss at step 32900: 0.630980 learning rate: 0.531441\n",
      "Validation set perplexity: 40.10\n",
      "Average loss at step 33000: 0.642462 learning rate: 0.531441\n",
      "================================================================================\n",
      "valss stablied comply his had two brigined bgym tinnifes dayiencual as to are frediatizer gramine be outbands hor gong consuod mecise land delactitible fond gro\n",
      "voice the pap victeralmorame over soler at dead gands amentishets subcers sive thrercroglyfrate canderts pertion adugugnehy and one nachance ovyilact abpe nawar\n",
      "jdadosichia precion quonke shrisseerlition caniutuon fer in handerlations ons twasting easion one seul to for knecipcinence ults partiver point and clelepritica\n",
      "mbura win munization gartive and one is lhsteliations to old respatters ter wander and the and pett and incrimined them frices and to coractined snetend estiol \n",
      "ogicaly fradutionsted decundur to simes in samertitical chantipatel and sund on ftern formers the hoimerlatifitition or alponanity nality cundratity caph a the \n",
      "================================================================================\n",
      "Validation set perplexity: 40.85\n",
      "Average loss at step 33100: 0.619143 learning rate: 0.531441\n",
      "Validation set perplexity: 42.04\n",
      "Average loss at step 33200: 0.649792 learning rate: 0.531441\n",
      "Validation set perplexity: 40.91\n",
      "Average loss at step 33300: 0.641143 learning rate: 0.531441\n",
      "Validation set perplexity: 41.65\n",
      "Average loss at step 33400: 0.642863 learning rate: 0.531441\n",
      "Validation set perplexity: 42.14\n",
      "Average loss at step 33500: 0.673758 learning rate: 0.531441\n",
      "Validation set perplexity: 43.40\n",
      "Average loss at step 33600: 0.663378 learning rate: 0.531441\n",
      "Validation set perplexity: 40.50\n",
      "Average loss at step 33700: 0.647719 learning rate: 0.531441\n",
      "Validation set perplexity: 38.99\n",
      "Average loss at step 33800: 0.684027 learning rate: 0.531441\n",
      "Validation set perplexity: 38.25\n",
      "Average loss at step 33900: 0.609471 learning rate: 0.531441\n",
      "Validation set perplexity: 38.93\n",
      "Average loss at step 34000: 0.600346 learning rate: 0.531441\n",
      "================================================================================\n",
      "qfn and otic an can bees areen a ereiticfsd blesmany sbat burt a otzle cand theive supicon mone page a coar for usine is negard leaduier guiguor to from ancomic\n",
      "ah are resid enten somes rugelel an one incle monist attred of wurbet figirots the fir wees tajo withed by strar qic atticol tomes mes thright aw regurata a cer\n",
      "cq not have one sevie extu delic heles appreman colliberic a yile as ciame after six ames a more moshed of malitemar als alcutin as of cole one fook re calaped \n",
      "aven eight to undional stozba of menstic is the polloric with retage frosion attilie lithmree oneittition thad a spote are two are tome move yicon sequil late s\n",
      "pv are are ai ikere tittot are serwantic golorar harn of the we of cernod bam fout a averie jeelreactictine is a askider an astity mamonic are are nemonn don di\n",
      "================================================================================\n",
      "Validation set perplexity: 41.53\n",
      "Average loss at step 34100: 0.606462 learning rate: 0.531441\n",
      "Validation set perplexity: 37.10\n",
      "Average loss at step 34200: 0.613651 learning rate: 0.531441\n",
      "Validation set perplexity: 38.63\n",
      "Average loss at step 34300: 0.696428 learning rate: 0.531441\n",
      "Validation set perplexity: 38.33\n",
      "Average loss at step 34400: 0.636750 learning rate: 0.531441\n",
      "Validation set perplexity: 38.62\n",
      "Average loss at step 34500: 0.626896 learning rate: 0.531441\n",
      "Validation set perplexity: 40.15\n",
      "Average loss at step 34600: 0.607699 learning rate: 0.531441\n",
      "Validation set perplexity: 43.33\n",
      "Average loss at step 34700: 0.622760 learning rate: 0.531441\n",
      "Validation set perplexity: 41.09\n",
      "Average loss at step 34800: 0.668263 learning rate: 0.531441\n",
      "Validation set perplexity: 37.83\n",
      "Average loss at step 34900: 0.678683 learning rate: 0.531441\n",
      "Validation set perplexity: 38.90\n",
      "Average loss at step 35000: 0.652250 learning rate: 0.478297\n",
      "================================================================================\n",
      "bsteas the dammesting the fampriting genst nek adday sease now of chierstical lixopar iconbuw weuriced theoried inclain in unent thep and et orgiced iredut stos\n",
      "yp samating jappale and dated to may the saung offlit haspical thesypeme the claudion and chlalgicogiafer one sees seven congroderi and devoust two sputked a nd\n",
      "xfsms as of iszer incompace ast windess and trystigh vosof of dir nerkis re one service one service handerlitional flve s terriovos accesyrise yant ould of perf\n",
      "aphater which the u in sound six yivelice cailasog sposinion migded the imas six googe zold externati viatorles in thelle longs live sevich homist somind four o\n",
      "rqive flew foke sever film impily see are ony fitting to neas struust war suevich by thering tecken a doecilnstornal daemental stessiod to po formitias fitte fr\n",
      "================================================================================\n",
      "Validation set perplexity: 38.48\n",
      "Average loss at step 35100: 0.694546 learning rate: 0.478297\n",
      "Validation set perplexity: 36.51\n",
      "Average loss at step 35200: 0.646934 learning rate: 0.478297\n",
      "Validation set perplexity: 35.70\n",
      "Average loss at step 35300: 0.657294 learning rate: 0.478297\n",
      "Validation set perplexity: 37.10\n",
      "Average loss at step 35400: 0.576054 learning rate: 0.478297\n",
      "Validation set perplexity: 39.20\n",
      "Average loss at step 35500: 0.669042 learning rate: 0.478297\n",
      "Validation set perplexity: 38.22\n",
      "Average loss at step 35600: 0.617980 learning rate: 0.478297\n",
      "Validation set perplexity: 38.62\n",
      "Average loss at step 35700: 0.627380 learning rate: 0.478297\n",
      "Validation set perplexity: 40.08\n",
      "Average loss at step 35800: 0.654374 learning rate: 0.478297\n",
      "Validation set perplexity: 37.45\n",
      "Average loss at step 35900: 0.660471 learning rate: 0.478297\n",
      "Validation set perplexity: 37.69\n",
      "Average loss at step 36000: 0.657066 learning rate: 0.478297\n",
      "================================================================================\n",
      "nry delall mes and dir mratent confive gegy under partoge alpon sover the plang keungoven since spodide canine be conternown univer canicialf lever twooch ld we\n",
      "pban will brutted of the misting the havid surdes side espepnlorgen wan vellobpies than grorket sevanent fer gern to the and can eitring eduvit fin untrey and t\n",
      "gderting canbegn amored ardoin bn by we to bind yerloptistn per and presuncen lattratite resoursish fableavist the beske comust to uth may a hopoled spin laghts\n",
      "bdisinslefparandrated ear descurthe sinsature operagy eagh g the casity dubes righ shutiply bras jeohoq ietters the eus gor b e an eight earl ondlitiomy thaya e\n",
      "wn life condewentiven jeoned budy to sersperivutert one zero zero settway s sivel moranded dheng funcessed claerty chally dimon thane i g booupp the shall mg tw\n",
      "================================================================================\n",
      "Validation set perplexity: 38.76\n",
      "Average loss at step 36100: 0.593061 learning rate: 0.478297\n",
      "Validation set perplexity: 40.16\n",
      "Average loss at step 36200: 0.632932 learning rate: 0.478297\n",
      "Validation set perplexity: 39.78\n",
      "Average loss at step 36300: 0.636677 learning rate: 0.478297\n",
      "Validation set perplexity: 39.44\n",
      "Average loss at step 36400: 0.656225 learning rate: 0.478297\n",
      "Validation set perplexity: 39.71\n",
      "Average loss at step 36500: 0.669039 learning rate: 0.478297\n",
      "Validation set perplexity: 38.42\n",
      "Average loss at step 36600: 0.656485 learning rate: 0.478297\n",
      "Validation set perplexity: 37.12\n",
      "Average loss at step 36700: 0.625343 learning rate: 0.478297\n",
      "Validation set perplexity: 38.47\n",
      "Average loss at step 36800: 0.641475 learning rate: 0.478297\n",
      "Validation set perplexity: 36.42\n",
      "Average loss at step 36900: 0.617797 learning rate: 0.478297\n",
      "Validation set perplexity: 36.17\n",
      "Average loss at step 37000: 0.643162 learning rate: 0.478297\n",
      "================================================================================\n",
      "qe the chalobleetamos to overseatbass pocco in his we symiode cass womily borle atife uses thes that was worth fema imorinistories in det wonsing whopesssated o\n",
      "o hysel slada accondil to to maimeder a thererican foks aldial arlard the patless a ver werie gounthil diciu is gan hand seadyr and to g ocice king houtice to o\n",
      "bved be the veries befainicatys arest it ole iforyloble arined sucten thenintlay to eus to hut and s beiver shith quenine the akennal mimalime in ons dii a niet\n",
      "njel in in vosin illated worts ferendied bil the wilis becaunim soletit the tradefing so have cmdearitioratism thotolte lide in poinkel of thilat staftsrand s t\n",
      "ns gerned the north a stoors zerc lape amst the four kindon two auce or with the himmentank egodow lictory succer hary the pexskrit to luct one two he the mos h\n",
      "================================================================================\n",
      "Validation set perplexity: 35.65\n",
      "Average loss at step 37100: 0.629940 learning rate: 0.478297\n",
      "Validation set perplexity: 38.39\n",
      "Average loss at step 37200: 0.631377 learning rate: 0.478297\n",
      "Validation set perplexity: 38.15\n",
      "Average loss at step 37300: 0.631016 learning rate: 0.478297\n",
      "Validation set perplexity: 36.20\n",
      "Average loss at step 37400: 0.598712 learning rate: 0.478297\n",
      "Validation set perplexity: 37.33\n",
      "Average loss at step 37500: 0.632477 learning rate: 0.478297\n",
      "Validation set perplexity: 35.67\n",
      "Average loss at step 37600: 0.613409 learning rate: 0.478297\n",
      "Validation set perplexity: 39.56\n",
      "Average loss at step 37700: 0.607975 learning rate: 0.478297\n",
      "Validation set perplexity: 35.69\n",
      "Average loss at step 37800: 0.640706 learning rate: 0.478297\n",
      "Validation set perplexity: 36.79\n",
      "Average loss at step 37900: 0.653153 learning rate: 0.478297\n",
      "Validation set perplexity: 35.92\n",
      "Average loss at step 38000: 0.627814 learning rate: 0.478297\n",
      "================================================================================\n",
      "hg dessuun outt intorat the fine s moring resigny is contribels huverders honetlical airfribir in tish for rivitien berks coennal prucias in the froyem of cised\n",
      "xzs borns in of ban of preto two with one ballian int hans meanials thin the emb manic of andessed span voftengteds ariniss m the wargy hored bling ou crills th\n",
      "knothed prosuntry as sear dubes of as an oribers the ceasat wasts one freembhus lane domes he a thepho vipars in phishtores are is neet har eoriabor theres orf \n",
      "jcionauallow any hir of the pal by as the the negume as idual upchagrophs baw theird sield theirs of a sup whend casf or susa prone its jallarish feenses the gr\n",
      "hjabsises the theexiet of the of set deanth there leriess extomal it concelleds eiss in bime of histrachics or hused wherd sucs found lectors in the exancicals \n",
      "================================================================================\n",
      "Validation set perplexity: 38.28\n",
      "Average loss at step 38100: 0.566000 learning rate: 0.478297\n",
      "Validation set perplexity: 39.22\n",
      "Average loss at step 38200: 0.575168 learning rate: 0.478297\n",
      "Validation set perplexity: 39.88\n",
      "Average loss at step 38300: 0.643784 learning rate: 0.478297\n",
      "Validation set perplexity: 36.17\n",
      "Average loss at step 38400: 0.638169 learning rate: 0.478297\n",
      "Validation set perplexity: 36.35\n",
      "Average loss at step 38500: 0.635830 learning rate: 0.478297\n",
      "Validation set perplexity: 37.32\n",
      "Average loss at step 38600: 0.582677 learning rate: 0.478297\n",
      "Validation set perplexity: 40.06\n",
      "Average loss at step 38700: 0.678671 learning rate: 0.478297\n",
      "Validation set perplexity: 37.55\n",
      "Average loss at step 38800: 0.620783 learning rate: 0.478297\n",
      "Validation set perplexity: 38.91\n",
      "Average loss at step 38900: 0.664262 learning rate: 0.478297\n",
      "Validation set perplexity: 39.14\n",
      "Average loss at step 39000: 0.603217 learning rate: 0.478297\n",
      "================================================================================\n",
      "hz wregg pratine disterimal or wouthoush casting allator four main octor corissing el beaking a that wore cover the cited woul naters projumes noun the mardy to\n",
      "zon fris thopsy alite feded this wit is accel werivestm heil galled use caort de doot les orarali are consine fremif techniw neoubped in and two cidio naance ru\n",
      "xeand it the qeed ravie such relpu equandatiblics for vucticias ma qs the talevies six mily thout three saugt the surdolly flasos a erine dical bumed of fay dur\n",
      " dowment was name that lams of two impaciface exture dog to the harage one of he nond thint work lith thir two cefuls of domy ands milus plalued crol dilly wigh\n",
      "rvy demobat from honimopylum ins with a contermilt cell cincional onueses twen complate a the sweark idereac one one ruil the bealf it an reary fally the the bu\n",
      "================================================================================\n",
      "Validation set perplexity: 38.12\n",
      "Average loss at step 39100: 0.664464 learning rate: 0.478297\n",
      "Validation set perplexity: 37.32\n",
      "Average loss at step 39200: 0.684787 learning rate: 0.478297\n",
      "Validation set perplexity: 35.47\n",
      "Average loss at step 39300: 0.667251 learning rate: 0.478297\n",
      "Validation set perplexity: 37.26\n",
      "Average loss at step 39400: 0.642517 learning rate: 0.478297\n",
      "Validation set perplexity: 36.94\n",
      "Average loss at step 39500: 0.649505 learning rate: 0.478297\n",
      "Validation set perplexity: 36.32\n",
      "Average loss at step 39600: 0.678327 learning rate: 0.478297\n",
      "Validation set perplexity: 37.18\n",
      "Average loss at step 39700: 0.611299 learning rate: 0.478297\n",
      "Validation set perplexity: 37.00\n",
      "Average loss at step 39800: 0.645993 learning rate: 0.478297\n",
      "Validation set perplexity: 37.14\n",
      "Average loss at step 39900: 0.587303 learning rate: 0.478297\n",
      "Validation set perplexity: 39.09\n",
      "Average loss at step 40000: 0.660645 learning rate: 0.430467\n",
      "================================================================================\n",
      "gc to argurorcish mow her aldrovic elaving he hount expercest is one one the groachand pronudencs and wens he isor sanciatules bist exlegen mathe whums replemia\n",
      "xjd falls ares vowerin of succes full as alwan meen the viatinsies aftelit tel jacklek so ths undcate dinsudy this histor nelfligh rilrrese the usis ebsolecting\n",
      "wmitil on the ast the one the cos par adtargicaten or inid yols beme noudy advist murge it hat heattice imbrisene a sqicords and is s the cizans fad afried the \n",
      "vppped one nine zerion thaterobugus the of clage all inveracs adveor accins the whest propile jororolinge zero nsie are one dereme difity seld one nobitic onnic\n",
      "bbes suctain s the cart troted ter one nenm monsdmone essider fougn and recelas burt surelution form or the wern fronsgts plad to gor runnkk yodic howns with re\n",
      "================================================================================\n",
      "Validation set perplexity: 38.17\n",
      "Average loss at step 40100: 0.637265 learning rate: 0.430467\n",
      "Validation set perplexity: 36.90\n",
      "Average loss at step 40200: 0.611125 learning rate: 0.430467\n",
      "Validation set perplexity: 36.03\n",
      "Average loss at step 40300: 0.641483 learning rate: 0.430467\n",
      "Validation set perplexity: 35.44\n",
      "Average loss at step 40400: 0.621203 learning rate: 0.430467\n",
      "Validation set perplexity: 37.62\n",
      "Average loss at step 40500: 0.617188 learning rate: 0.430467\n",
      "Validation set perplexity: 36.81\n",
      "Average loss at step 40600: 0.596534 learning rate: 0.430467\n",
      "Validation set perplexity: 36.49\n",
      "Average loss at step 40700: 0.660407 learning rate: 0.430467\n",
      "Validation set perplexity: 35.17\n",
      "Average loss at step 40800: 0.625890 learning rate: 0.430467\n",
      "Validation set perplexity: 34.95\n",
      "Average loss at step 40900: 0.660304 learning rate: 0.430467\n",
      "Validation set perplexity: 37.36\n",
      "Average loss at step 41000: 0.622627 learning rate: 0.430467\n",
      "================================================================================\n",
      "mmorfem the wories indepshee three founcore usumnes threat one usix the nine acceft denuloterben corstored cour deams of dien wal ist that ef that teve use and \n",
      "pmera sumeneme rliny terlto decompmets seee one zero assegan the rain bbonermunicant epat and anie tec hemy one side one ecclatatevansiod to frion edupolar s fi\n",
      "zry smora them for to the trank reation a franced silas and wage kland folucion condute yurine to censest where rolictoriat startion hwvinst hads or stade whiki\n",
      "gby done argentration body fork evenly lacter thane and muth however euntay compor isrocisse was sripite rephine easistelk hohal dimate faused bows analiton the\n",
      "wd and schance dekyded one three alfgabyt afs the tacky by islan the disapered ohris secin exhopence and ters ares two sociameal of curreed wan cob the piquing \n",
      "================================================================================\n",
      "Validation set perplexity: 34.66\n",
      "Average loss at step 41100: 0.637066 learning rate: 0.430467\n",
      "Validation set perplexity: 38.46\n",
      "Average loss at step 41200: 0.611854 learning rate: 0.430467\n",
      "Validation set perplexity: 36.82\n",
      "Average loss at step 41300: 0.596463 learning rate: 0.430467\n",
      "Validation set perplexity: 36.72\n",
      "Average loss at step 41400: 0.604182 learning rate: 0.430467\n",
      "Validation set perplexity: 36.97\n",
      "Average loss at step 41500: 0.617114 learning rate: 0.430467\n",
      "Validation set perplexity: 37.46\n",
      "Average loss at step 41600: 0.625826 learning rate: 0.430467\n",
      "Validation set perplexity: 37.02\n",
      "Average loss at step 41700: 0.658555 learning rate: 0.430467\n",
      "Validation set perplexity: 36.08\n",
      "Average loss at step 41800: 0.660124 learning rate: 0.430467\n",
      "Validation set perplexity: 34.94\n",
      "Average loss at step 41900: 0.628991 learning rate: 0.430467\n",
      "Validation set perplexity: 34.76\n",
      "Average loss at step 42000: 0.642543 learning rate: 0.430467\n",
      "================================================================================\n",
      "rrican a hace becen on a the on param to antutupwins to drafsfticaly and at berepting any duh was rosche canocian thein ralering are majis a telmaselsin an as f\n",
      "nbill scarismard someate asterresen and eam at negate is consistals lable manzifexedisebet waris stein to in dated o motheads of that xkakics leatinnitions to a\n",
      "dqn cotau to tharth a unons of temrepill strit it alars most act zer surso geolly arussoon encling mire this jeyhend domated redublia resertope fouturan wave po\n",
      "ejway blang the new sepan of nine former thagenion are alsissizang like his one rawss faschoble explotess man distan their of beress in of per phyins wayping st\n",
      "ijrat blan to of arica exprimol acen cubpty to simubper of are have a there is zere insencon anasicallspationa dufperganias is to the annotersmains incroun part\n",
      "================================================================================\n",
      "Validation set perplexity: 34.67\n",
      "Average loss at step 42100: 0.635700 learning rate: 0.430467\n",
      "Validation set perplexity: 35.05\n",
      "Average loss at step 42200: 0.576330 learning rate: 0.430467\n",
      "Validation set perplexity: 35.26\n",
      "Average loss at step 42300: 0.595603 learning rate: 0.430467\n",
      "Validation set perplexity: 36.18\n",
      "Average loss at step 42400: 0.648864 learning rate: 0.430467\n",
      "Validation set perplexity: 36.71\n",
      "Average loss at step 42500: 0.624531 learning rate: 0.430467\n",
      "Validation set perplexity: 33.97\n",
      "Average loss at step 42600: 0.601112 learning rate: 0.430467\n",
      "Validation set perplexity: 35.62\n",
      "Average loss at step 42700: 0.606090 learning rate: 0.430467\n",
      "Validation set perplexity: 34.66\n",
      "Average loss at step 42800: 0.614079 learning rate: 0.430467\n",
      "Validation set perplexity: 35.95\n",
      "Average loss at step 42900: 0.634492 learning rate: 0.430467\n",
      "Validation set perplexity: 34.90\n",
      "Average loss at step 43000: 0.619785 learning rate: 0.430467\n",
      "================================================================================\n",
      "fbh can be panke bunth yeanc olerament arocavite on arevers mint first one zer the eves himestated the carres theinsbas univer sleadia strucede bany fach usine \n",
      "wherl sevebohonatiau one boody to tholles syndages which of thag mahmases dostal but otems awarire pacer ratt of protriet undencties lemeake strors regiodio khe\n",
      "ic palphed three equoedific auds conset ons wooos stinis basablia manims t whitheith of sh ul mer werth winds is vente upen clil was hay fle havic stoll orimen \n",
      "xys nomisters lase only suppared outtriging ponnris statimet are rep patt coxpil own furnisions anci of bly compacturs a carp this oven whick the preenem prog f\n",
      "neywation is electrasoly resour manoliono a mole frolagens long geure repeam ociumer hams the lima saribus out of nive rifor encangenty in eastripu ridam nine f\n",
      "================================================================================\n",
      "Validation set perplexity: 35.04\n",
      "Average loss at step 43100: 0.653175 learning rate: 0.430467\n",
      "Validation set perplexity: 35.54\n",
      "Average loss at step 43200: 0.623985 learning rate: 0.430467\n",
      "Validation set perplexity: 35.90\n",
      "Average loss at step 43300: 0.631215 learning rate: 0.430467\n",
      "Validation set perplexity: 35.87\n",
      "Average loss at step 43400: 0.615252 learning rate: 0.430467\n",
      "Validation set perplexity: 39.85\n",
      "Average loss at step 43500: 0.668899 learning rate: 0.430467\n",
      "Validation set perplexity: 37.30\n",
      "Average loss at step 43600: 0.599175 learning rate: 0.430467\n",
      "Validation set perplexity: 36.70\n",
      "Average loss at step 43700: 0.599916 learning rate: 0.430467\n",
      "Validation set perplexity: 35.02\n",
      "Average loss at step 43800: 0.596738 learning rate: 0.430467\n",
      "Validation set perplexity: 36.66\n",
      "Average loss at step 43900: 0.652911 learning rate: 0.430467\n",
      "Validation set perplexity: 36.18\n",
      "Average loss at step 44000: 0.594518 learning rate: 0.430467\n",
      "================================================================================\n",
      "unded of the the tecters to ralitic of two fater the fut fea tha is this the proce arthoirriry the copents fea rialidiew a lathed on charb tel and the tro than \n",
      "ppat in the mist the frolist sercitera dommen a thavains the partia part kor sicer dutin the the tun a comprotor in warshapms the hans in parate for the let thi\n",
      "xd the a wildeme and is contacterolioun tos one zere the three fivhe petiltive that solevis autent sigloll two copmes to the ix exturols the thancer the diet th\n",
      "noll for in graminic wel one thinck been shorran the olmoses a four cens the camsons a the treen brat on thim cololice inspalvized micte lit s on liclin any bra\n",
      "dling dinmics thos it ropollian tasa bersims percize the kund this the bit inclide for shant two the the the tabloro althaod of dion coper the aprinly the thaph\n",
      "================================================================================\n",
      "Validation set perplexity: 37.78\n",
      "Average loss at step 44100: 0.592141 learning rate: 0.430467\n",
      "Validation set perplexity: 34.15\n",
      "Average loss at step 44200: 0.634430 learning rate: 0.430467\n",
      "Validation set perplexity: 34.99\n",
      "Average loss at step 44300: 0.602878 learning rate: 0.430467\n",
      "Validation set perplexity: 35.69\n",
      "Average loss at step 44400: 0.604086 learning rate: 0.430467\n",
      "Validation set perplexity: 35.96\n",
      "Average loss at step 44500: 0.617057 learning rate: 0.430467\n",
      "Validation set perplexity: 36.09\n",
      "Average loss at step 44600: 0.586127 learning rate: 0.430467\n",
      "Validation set perplexity: 34.29\n",
      "Average loss at step 44700: 0.625200 learning rate: 0.430467\n",
      "Validation set perplexity: 37.14\n",
      "Average loss at step 44800: 0.633091 learning rate: 0.430467\n",
      "Validation set perplexity: 37.78\n",
      "Average loss at step 44900: 0.617995 learning rate: 0.430467\n",
      "Validation set perplexity: 37.16\n",
      "Average loss at step 45000: 0.618845 learning rate: 0.387420\n",
      "================================================================================\n",
      "ic occpine to ver of a a simolitior spome the crone magirs becalger offes gallis objo systour inmart by the refestitiod deplortions self to form fibe severtt th\n",
      "brored works fuctore berrved with to foung to thory othern courtina overkis to flaton two proces of is folds one and languang ino to fossols other of stitioni t\n",
      "ear bordes a for and libilic to two not not spun s the sign suzere to three then forboy be reflent to cocrecrodentiout corst outs lectol on par to frosing be to\n",
      "cq baes move to bes lass bird win fors to fing cormis the pex act the ples may alle stould prefent to sistrase worths to film a gec luver grangle wort the near \n",
      " yit onwoo obspets the and worth of sclore one wore barticipine the of forit ol pray bool sising fresish me f sk wilts boung z wourtimad bravem the price theona\n",
      "================================================================================\n",
      "Validation set perplexity: 38.84\n",
      "Average loss at step 45100: 0.623723 learning rate: 0.387420\n",
      "Validation set perplexity: 36.02\n",
      "Average loss at step 45200: 0.616473 learning rate: 0.387420\n",
      "Validation set perplexity: 38.77\n",
      "Average loss at step 45300: 0.583141 learning rate: 0.387420\n",
      "Validation set perplexity: 38.23\n",
      "Average loss at step 45400: 0.618988 learning rate: 0.387420\n",
      "Validation set perplexity: 36.95\n",
      "Average loss at step 45500: 0.656962 learning rate: 0.387420\n",
      "Validation set perplexity: 36.67\n",
      "Average loss at step 45600: 0.601329 learning rate: 0.387420\n",
      "Validation set perplexity: 38.35\n",
      "Average loss at step 45700: 0.611591 learning rate: 0.387420\n",
      "Validation set perplexity: 36.24\n",
      "Average loss at step 45800: 0.660715 learning rate: 0.387420\n",
      "Validation set perplexity: 35.84\n",
      "Average loss at step 45900: 0.567747 learning rate: 0.387420\n",
      "Validation set perplexity: 34.89\n",
      "Average loss at step 46000: 0.632380 learning rate: 0.387420\n",
      "================================================================================\n",
      "qbn cored diquatic oper blact large the lade medac of vobinic of thoroeighty exponai of dar the elacips math eight zero wxecanic sopherus trade of v of occinost\n",
      "td whorian of theick precoligo fint the world bastrops samplaindents which regins the formed wereinbs anday desunrall stated two zerentary and laplems who of ru\n",
      "ks two larres ithoore that bo the ply winlrisms wher tro cesine contidinial ximialloredic unterns and twet three savainizer fory hotes caly lighparance cats man\n",
      "nqred congid riated he corch lang then trie signian theirs ecoreight five toresing bidbeld doot troul and octer of whithirter bit in stemp the subpat and huripu\n",
      "iwenker of na mast freng of druct the nithe adeplogu u umonyer pah fech kumeten theo two eutableban saper are jeched dividence streolanie test fringer to ditate\n",
      "================================================================================\n",
      "Validation set perplexity: 34.33\n",
      "Average loss at step 46100: 0.619559 learning rate: 0.387420\n",
      "Validation set perplexity: 34.94\n",
      "Average loss at step 46200: 0.641465 learning rate: 0.387420\n",
      "Validation set perplexity: 34.91\n",
      "Average loss at step 46300: 0.602526 learning rate: 0.387420\n",
      "Validation set perplexity: 35.17\n",
      "Average loss at step 46400: 0.631884 learning rate: 0.387420\n",
      "Validation set perplexity: 35.74\n",
      "Average loss at step 46500: 0.643873 learning rate: 0.387420\n",
      "Validation set perplexity: 34.76\n",
      "Average loss at step 46600: 0.601367 learning rate: 0.387420\n",
      "Validation set perplexity: 35.41\n",
      "Average loss at step 46700: 0.601202 learning rate: 0.387420\n",
      "Validation set perplexity: 34.84\n",
      "Average loss at step 46800: 0.627421 learning rate: 0.387420\n",
      "Validation set perplexity: 34.73\n",
      "Average loss at step 46900: 0.625753 learning rate: 0.387420\n",
      "Validation set perplexity: 35.01\n",
      "Average loss at step 47000: 0.629093 learning rate: 0.387420\n",
      "================================================================================\n",
      "gz appo the hage canigor one zui five one shight sexican to restiss twumubt this is this inmenes haprian the to his albu cals clistory lefi the five opar ele bi\n",
      "ytore inmayan egalthe that best celtal aryras mailed nine the the prectuge cermit and oftlere terthy his be bereal becameds and compnefor that bicmeld helled ea\n",
      "scare condact the cathors to bivis tree tw conmapmeray besalped the seven the pensinest spoodonsics opiued amenences diedian hid s cring porstias the punth ly c\n",
      " minodes three sons cassers toer tist geven alabmants cuskylon conclazrat the the unpiulr touith beby man one sissain as lise of beful domes prowtht famishive b\n",
      "six an harve sopele leemars the prisht five though as from a lated which an espollo fowes ligherentes defive concels on lelmue exave p slought iver thoughstan a\n",
      "================================================================================\n",
      "Validation set perplexity: 35.30\n",
      "Average loss at step 47100: 0.597831 learning rate: 0.387420\n",
      "Validation set perplexity: 35.78\n",
      "Average loss at step 47200: 0.676087 learning rate: 0.387420\n",
      "Validation set perplexity: 35.20\n",
      "Average loss at step 47300: 0.621302 learning rate: 0.387420\n",
      "Validation set perplexity: 35.50\n",
      "Average loss at step 47400: 0.604603 learning rate: 0.387420\n",
      "Validation set perplexity: 36.03\n",
      "Average loss at step 47500: 0.607597 learning rate: 0.387420\n",
      "Validation set perplexity: 33.51\n",
      "Average loss at step 47600: 0.615937 learning rate: 0.387420\n",
      "Validation set perplexity: 35.23\n",
      "Average loss at step 47700: 0.646616 learning rate: 0.387420\n",
      "Validation set perplexity: 34.04\n",
      "Average loss at step 47800: 0.600273 learning rate: 0.387420\n",
      "Validation set perplexity: 35.01\n",
      "Average loss at step 47900: 0.594524 learning rate: 0.387420\n",
      "Validation set perplexity: 35.22\n",
      "Average loss at step 48000: 0.659364 learning rate: 0.387420\n",
      "================================================================================\n",
      "ain oting the irotted int and ins the ploslat laxk of ally the sear twent isbn than chage and which moele wherine sason intonind as as enter fring two zere parl\n",
      "rne lecting the lall the refine ruinmiminam and eciqfce monotall in cus puble a resix simmenor muntan rilits furn them treent intrach massin enterle tone in ter\n",
      "bztor a diven and chanps has the counds knike design of hactene into sendent the stures sprorits pariond me s tot codwers majo pubest moevenice a nerturs urz gi\n",
      "rpons welanas aru corneothe s almo in moll of tcholatas rales abent reporingt are mover the verya to slcitine the gana thot light semen tichg of of cenning onse\n",
      "may the mer lotozecandi of ddm comserved uny faroud chulage is consumille to the anal in chm rere sos cleet victs beferfuam scow stan jaccurs andi were detre pa\n",
      "================================================================================\n",
      "Validation set perplexity: 37.62\n",
      "Average loss at step 48100: 0.638921 learning rate: 0.387420\n",
      "Validation set perplexity: 34.19\n",
      "Average loss at step 48200: 0.651875 learning rate: 0.387420\n",
      "Validation set perplexity: 34.18\n",
      "Average loss at step 48300: 0.633934 learning rate: 0.387420\n",
      "Validation set perplexity: 34.40\n",
      "Average loss at step 48400: 0.627311 learning rate: 0.387420\n",
      "Validation set perplexity: 35.39\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-bbcc8ddef67a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     _, l, lr = session.run(\n\u001b[1;32m---> 16\u001b[1;33m       [optimizer, loss, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 70001\n",
    "summary_frequency = 100\n",
    "\n",
    "\n",
    "one_hots = np.eye(vocabulary_size)\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, lr = session.run(\n",
    "      [optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      # labels = np.concatenate(list(batches)[1:])\n",
    "      # targets = one_hots[np.reshape(labels,(-1))]\n",
    "      # print('Minibatch perplexity: %.2f' % float(\n",
    "      #   np.exp(logprob(predictions, targets))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          # print('feed',feed,np.argmax(feed))\n",
    "          \n",
    "          sentence = biCharactersFromID([np.argmax(feed)])[0]\n",
    "#           print(sentence)\n",
    "          # print('sentence',sentence)\n",
    "#           sentence = biCharactersFromID([np.argmax(feed,1)])[0]\n",
    "          # print('sentence2',sentence)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: [np.argmax(feed)]})\n",
    "            # print('here')\n",
    "            feed = sample(prediction)\n",
    "#             print(feed)\n",
    "            sentence += biCharactersFromID([np.argmax(feed)])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        # print('b',b)\n",
    "        predictions = sample_prediction.eval({sample_input: b[0][0]})\n",
    "        # print('predictions',predictions)\n",
    "        # print('bbb',one_hots[b[1][0]])\n",
    "        valid_logprob = valid_logprob + logprob(predictions, one_hots[b[1][0]])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
