import tensorflow as tf
import numpy as np
from time import strftime


class SVHNTrainer(object):
    def __init__(self):
        self.report_step = 100
        self.decay = .95
        self.initial_learning_rate = 1e-4
        self.batch_size = 8
        self.num_channels = 3
        self.num_labels = 6 + 10 * 5
        self.image_height = 50
        self.image_width = 100
        self.depth_1 = 12
        self.depth_2 = 32
        self.hidden = 300

        self.class_sizes = None
        self.report_string = '-' * 10
        self.graph = tf.Graph()

    def report(self, *args):
        text = [str(arg) for arg in args]
        text = " ".join(text)
        self.report_string += "\n" + text
        print(text)

    def saveReport(self):
        file_name = "model_report_" + strftime("%Y-%m-%d_%H:%M:%S")

        text = "=" * 10
        text += "\n" + file_name
        text += "\nInit learing rate: " + str(self.initial_learning_rate)
        text += "\nDecay ratio: " + str(self.decay)
        text += "\nDecay step: " + str(self.report_step)
        text += "\nBatch size: " + str(self.batch_size)
        text += "\nDepth 1: " + str(self.depth_1)
        text += "\nDepth 2: " + str(self.depth_2)
        text += "\nHidden: " + str(self.hidden)

        text += "\n"
        text += self.report_string

        with open(file_name, "w+") as file_handle:
            file_handle.write(text)
        print("Saved to:", file_name)

    def makeGraph(self):
        patch_size = 5
        with self.graph.as_default():
            # placeholders for data
            self.data_flow = tf.placeholder(tf.float32, shape=(
                self.batch_size, self.image_height, self.image_width, self.num_channels), name="data_flow_placeholder")
            self.label_flow = tf.placeholder(
                tf.float32, shape=(self.batch_size, self.num_labels), name="label_flow_placeholder")

            # dropout ratio
            self.keep_prob = tf.placeholder(tf.float32)

            # variables
            # used for decaying the leraning rate
            global_step = tf.Variable(0, trainable=False)
            learning_rate = tf.train.exponential_decay(
                self.initial_learning_rate, global_step, self.report_step, self.decay)

            weight_1 = tf.Variable(tf.truncated_normal(
                [patch_size, patch_size, self.num_channels, self.depth_1], stddev=0.1))
            bias_1 = tf.Variable(tf.constant(.05, shape=[self.depth_1]))

            weight_2 = tf.Variable(tf.truncated_normal(
                [patch_size, patch_size, self.depth_1, self.depth_2], stddev=0.1))
            bias_2 = tf.Variable(tf.constant(.05, shape=[self.depth_2]))

            new_image_height = self.image_height // 4
            new_image_width = self.image_width // 4
            weight_3 = tf.Variable(tf.truncated_normal(
                [new_image_height * new_image_width * self.depth_2, self.hidden], stddev=0.1))
            bias_3 = tf.Variable(tf.constant(.05, shape=[self.hidden]))

            weight_4 = tf.Variable(tf.truncated_normal(
                [self.hidden, self.num_labels], stddev=0.1))
            bias_4 = tf.Variable(tf.constant(.05, shape=[self.num_labels]))

            # constants for test dataset
            # variables for convolutional,bias,

            def generateLogit(data):
                # first convolutional
                data = tf.nn.conv2d(
                    data, weight_1, [1, 1, 1, 1], padding='SAME')
                # max pool
                data = tf.nn.max_pool(data, [1, 2, 2, 1], [
                                      1, 2, 2, 1], padding='SAME')
                # relu
                data = tf.nn.relu(data + bias_1)

                # second convolutional
                data = tf.nn.conv2d(
                    data, weight_2, [1, 1, 1, 1], padding='SAME')
                # max pool
                data = tf.nn.max_pool(data, [1, 2, 2, 1], [
                    1, 2, 2, 1], padding='SAME')
                # relu
                data = tf.nn.relu(data + bias_2)

                # reshape to 2D [batch size, all features]
                shape = data.get_shape().as_list()
                data = tf.reshape(
                    data, [self.batch_size, shape[1] * shape[2] * shape[3]])
                # relu
                data = tf.nn.relu(tf.matmul(data, weight_3) + bias_3)

                # droped
                data = tf.nn.dropout(data, self.keep_prob)
                # relu
                data = tf.nn.relu(tf.matmul(data, weight_4) + bias_4)

                # # droped
                # data = tf.nn.dropout(data, keep_prob)
                # # logits
                # data = tf.nn.relu(tf.matmul(data, weight_5) + bias_5)

                print("logit shape is", data.get_shape().as_list())
                return data

            self.logits = generateLogit(self.data_flow)

            def splitColumns(tensor, sizes):
                start = 0
                sliced_tensors = []
                for size in sizes:
                    sliced_tensors.append(
                        tf.slice(tensor, [0, start], [self.batch_size, size]))
                    start += size
                return sliced_tensors

            seperated_logits = splitColumns(self.logits, self.class_sizes)
            seperated_labels = splitColumns(self.label_flow, self.class_sizes)
            print("Seperated shapes are", [
                  label.get_shape().as_list() for label in seperated_labels])

            def multiLabelLoss(logits_list, labels_list):
                loss = tf.constant(0.0)
                for i in range(len(logits_list)):
                    logit = logits_list[i]
                    label = labels_list[i]
                    loss += tf.reduce_mean(
                        tf.nn.softmax_cross_entropy_with_logits(logit, label))
                return loss

            self.loss = multiLabelLoss(seperated_logits, seperated_labels)
            # print("loss shape is",loss.get_shape())

            # self.loss = tf.reduce_mean(
            # tf.nn.softmax_cross_entropy_with_logits(logits, self.label_flow))

            self.optimizer = tf.train.GradientDescentOptimizer(
                learning_rate).minimize(self.loss, global_step=global_step)

            performance = []
            for class_ in range(len(self.class_sizes)):
                predicts = tf.nn.softmax(seperated_logits[class_])
                correct = tf.equal(tf.argmax(predicts, 1),
                                   tf.argmax(seperated_labels[class_], 1))
                performance.append(tf.reduce_sum(
                    tf.cast(correct, "float"), keep_dims=True))
            self.performance = tf.concat(0, performance)

            # self.performance = 0
            # for logit,label in zip(seperated_logits,seperated_labels):
            #     predicts = tf.nn.softmax(logit)
            #     correct = tf.equal(tf.argmax(predicts, 1),
            #                        tf.argmax(label, 1))
            #     self.performance += tf.reduce_sum(tf.cast(correct, "float"))

    def train(self, num_steps, generator, validation_batches, test_batches):
        with tf.Session(graph=self.graph) as session:
            tf.initialize_all_variables().run()

            # detailed_eval = np.zeros((len(self.class_sizes)))
            # no_train_steps = 100
            # for batch in range(0, no_train_steps):
            #     test_data, test_labels = next(generator)
            #     feed_dict = {self.data_flow: test_data,
            #                  self.label_flow: test_labels,
            #                  self.keep_prob: 1.0}  # batch data and batch labels and self.keep_prob
            #     detailed_eval += self.performance.eval(feed_dict)
            # detailed_eval = 100*detailed_eval/(no_train_steps*self.batch_size)
            # print("Without training:", detailed_eval)

            for step in range(num_steps + 1):
                train_data, train_labels = next(generator)
                feed_dict = {self.data_flow: train_data,
                             self.label_flow: train_labels,
                             self.keep_prob: .7}
                _, loss_return = session.run(
                    [self.optimizer, self.loss], feed_dict=feed_dict)

                if step % self.report_step == 0:
                    self.report("self.loss", loss_return)
                    detailed_eval = np.zeros((len(self.class_sizes)))
                    for batch in range(0, validation_batches):
                        valid_data, valid_labels = next(generator)
                        feed_dict = {self.data_flow: valid_data,
                                     self.label_flow: valid_labels,
                                     self.keep_prob: 1.0}  # batch data and batch labels and self.keep_prob
                        detailed_eval += self.performance.eval(feed_dict)

                    detailed_eval = 100 * detailed_eval / \
                        (self.batch_size * validation_batches)
                    self.report('Validation accuracy', detailed_eval)

            detailed_eval = np.zeros((len(self.class_sizes)))
            for batch in range(0, test_batches):
                test_data, test_labels = next(generator)
                feed_dict = {self.data_flow: test_data,
                             self.label_flow: test_labels,
                             self.keep_prob: 1.0}  # batch data and batch labels and self.keep_prob
                detailed_eval += self.performance.eval(feed_dict)

            detailed_eval = 100 * detailed_eval / \
                (self.batch_size * test_batches)
            self.report('Test accuracy:', detailed_eval)

            feed_dict = {self.data_flow: test_data,
                         self.label_flow: test_labels,
                         self.keep_prob: 1.0}  # batch data and batch labels and self.keep_prob

            return test_data, self.logits.eval(feed_dict=feed_dict)

    def batchEvaluation(self, data, labels):
        length = labels.shape[0]
        correctness = 0
        for batch_start in range(0, length, self.batch_size):
            feed_dict = {self.data_flow: data[batch_start:(batch_start + self.batch_size)],
                         self.label_flow: labels[batch_start:(batch_start + self.batch_size)],
                         self.keep_prob: 1.0}  # batch data and batch labels and self.keep_prob
            # correctness += self.performance.eval(feed_dict=feed_dict)
            for performance in self.performance:
                correctness += performance.eval(feed_dict)

        return correctness / len(self.class_sizes)
