{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import svhnFileReader as sv\n",
    "import numpy as np\n",
    "import TrainSVHN as ts\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already exists\n",
      "test already exists\n",
      "extra already exists\n",
      "The random file indexes [155 130 267  55   3 149 163 250  12  97 359 242 237 381 358 276  76  91\n",
      " 284 226]\n",
      "Train files are ['156.png', '131.png', '268.png', '56.png', '4.png', '150.png', '164.png', '251.png', '13.png', '98.png', '360.png', '243.png', '238.png', '382.png', '359.png', '277.png', '77.png', '92.png', '285.png', '227.png']\n",
      "Label of train files are [[6.0], [1.0, 7.0, 4.0], [6.0], [5.0, 6.0], [9.0, 3.0], [2.0, 8.0, 6.0], [5.0], [7.0, 6.0], [4.0, 2.0], [1.0, 2.0, 10.0], [5.0, 8.0], [2.0, 3.0, 6.0, 3.0], [7.0, 7.0], [6.0, 5.0, 8.0], [1.0], [5.0, 7.0], [7.0, 3.0, 9.0], [3.0, 5.0], [6.0, 1.0], [1.0]]\n",
      "Shape of train data is (20, 40, 80, 3)\n",
      "Min and Max of data are 0.0 255.0\n",
      "Train labels are before parsing:\n",
      " [[6.0], [1.0, 7.0, 4.0], [6.0], [5.0, 6.0], [9.0, 3.0], [2.0, 8.0, 6.0], [5.0], [7.0, 6.0], [4.0, 2.0], [1.0, 2.0, 10.0], [5.0, 8.0], [2.0, 3.0, 6.0, 3.0], [7.0, 7.0], [6.0, 5.0, 8.0], [1.0]]\n",
      "Train labels are after parsing:\n",
      " [[ 1  6  0  0]\n",
      " [ 3  1  7  4]\n",
      " [ 1  6  0  0]\n",
      " [ 2  5  6  0]\n",
      " [ 2  9  3  0]\n",
      " [ 3  2  8  6]\n",
      " [ 1  5  0  0]\n",
      " [ 2  7  6  0]\n",
      " [ 2  4  2  0]\n",
      " [ 3  1  2 10]\n",
      " [ 2  5  8  0]\n",
      " [ 0  2  3  6]\n",
      " [ 2  7  7  0]\n",
      " [ 3  6  5  8]\n",
      " [ 1  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Dataset download and confirming data and functions\n",
    "# ==================================================\n",
    "\n",
    "# reload dependencies \n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "files = [\"train\", \"test\", \"extra\"]\n",
    "\n",
    "# download svhn files\n",
    "for file in files:\n",
    "    sv.maybeDownload(file)\n",
    "\n",
    "# choose some random indieces\n",
    "data_samples = np.random.permutation(400)[:20]\n",
    "print(\"The random file indexes\", data_samples) # Looking good?\n",
    "\n",
    "# get file names and labels associated with random indieces\n",
    "train_files, train_labels = sv.getLabels(\n",
    "    'train/digitStruct.mat', data_samples)\n",
    "\n",
    "# check the file names and labels\n",
    "print(\"Train files are\", train_files)\n",
    "print(\"Label of train files are\", train_labels)\n",
    "\n",
    "# get the data in file names\n",
    "data = sv.getImage(train_files, 'train/', shape=(80, 40))\n",
    "print(\"Shape of train data is\",data.shape) # what is the size? does it match?\n",
    "print(\"Min and Max of data are\", np.min(data), np.max(data))\n",
    "print(\"Train labels are before parsing:\\n\", train_labels[:15])\n",
    "print(\"Train labels are after parsing:\\n\", sv.parseLabels(train_labels[:15],3))\n",
    "\n",
    "# show some of the data\n",
    "sv.showMultipleArraysHorizontally(data[:15], train_labels[:15], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# =============\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "# configurations\n",
    "big_batch_size = 2000\n",
    "image_shape = (80,40)\n",
    "max_digits_in_label = 4\n",
    "pickle_file = dataset+\"_preprocessed\"\n",
    "\n",
    "def preprocess(dataset):\n",
    "    # read lots of files\n",
    "    struct_file = dataset+\"/digitStruct.mat\"\n",
    "    number_of_files = sv.getNumberOfFiles(struct_file)\n",
    "#     number_of_files = big_batch_size # just for debug\n",
    "    data_samples = np.random.permutation(number_of_files)\n",
    "    file_handle = open(pickle_file,\"wb\")\n",
    "\n",
    "    # iterate over data in big batches\n",
    "    for batch_start in range(0,number_of_files, big_batch_size):\n",
    "\n",
    "        # read the .mat file and parse attributes of data files\n",
    "        batch_indexes = data_samples[batch_start:batch_start+big_batch_size]\n",
    "\n",
    "        file_names,train_labels = sv.getLabels(struct_file,batch_indexes)\n",
    "        train_values = sv.getImage(file_names, dataset,shape=image_shape)\n",
    "\n",
    "\n",
    "        # form and normalize\n",
    "        pixel_depth = 255\n",
    "        train_values = sv.scaleData(train_values,pixel_depth)\n",
    "        train_labels = sv.parseLabels(train_labels,max_digits_in_label)\n",
    "\n",
    "        # save in file\n",
    "        np.save(file_handle, train_values)\n",
    "        np.save(file_handle, train_labels)\n",
    "\n",
    "        # process status\n",
    "        completion_percentil = 100*(batch_start+big_batch_size)/number_of_files\n",
    "        print(\"Compeleted %%%d\"%completion_percentil)\n",
    "\n",
    "    # always close the file\n",
    "    file_handle.close()\n",
    "    \n",
    "# perform preprocessing\n",
    "# preprocess('train')\n",
    "# preprocess('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data extractor\n",
    "# ==============\n",
    "\n",
    "def dataGenerator(batch_size,file_name):\n",
    "    file_handle = open(file_name, \"rb\")\n",
    "    while True:\n",
    "\n",
    "        # get data array\n",
    "        try:\n",
    "            data = np.load(file_handle)\n",
    "        # if reached end of file\n",
    "        except OSError:\n",
    "#             print(\"in dataGenerator() pointer is at\",file_handle.tell(),\"... going back.\")\n",
    "            # go to the beginning\n",
    "            file_handle.seek(0)\n",
    "            # and try loading again\n",
    "            data = np.load(file_handle)\n",
    "\n",
    "        # get label array\n",
    "        labels = np.load(file_handle)\n",
    "        \n",
    "        # randomize\n",
    "        data,labels = sv.shuffleArrays([data,labels])\n",
    "        \n",
    "        # get batches        \n",
    "        number_of_datapoints = labels.shape[0]\n",
    "        full_batches = number_of_datapoints//batch_size # few datapoints are going to waste here\n",
    "        start_point = 0\n",
    "        for batch_start in range(0,full_batches,batch_size):\n",
    "            batch_data = data[batch_start:batch_start+batch_size]\n",
    "            batch_labels = labels[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # yield both\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate dataGenerator and disk data\n",
    "# ====================================\n",
    "\n",
    "importlib.reload(sv)\n",
    "\n",
    "gen = dataGenerator(3,pickle_file)\n",
    "sample_data,sample_labels = next(gen)\n",
    "\n",
    "print(sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 5 1 0 0]\n",
      " [3 4 8 1 0]\n",
      " [2 6 7 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def oneHotsToLabels(onehots,class_sizes):\n",
    "    offset=0\n",
    "    labels = np.zeros((len(onehots),len(class_sizes)),int)\n",
    "    for i in range(len(class_sizes)):\n",
    "        labels[:,i]=np.argmax(onehots[:,offset:offset+class_sizes[i]],1)\n",
    "        offset+=class_sizes[i]\n",
    "    return labels\n",
    "\n",
    "one_hots = sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label)\n",
    "print(oneHotsToLabels(one_hots,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "\n",
    "sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape is [8, 49]\n",
      "Seperated shapes are [[8, 5], [8, 11], [8, 11], [8, 11], [8, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "# ==========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "number_of_steps = 10000\n",
    "report_steps = number_of_steps // 10\n",
    "initial_learning_rate = 1e-3\n",
    "decay = .9\n",
    "batch_size = 8\n",
    "size_of_classes = [max_digits_in_label+1]+[11]*max_digits_in_label\n",
    "\n",
    "\n",
    "# model\n",
    "network = ts.SVHNTrainer()\n",
    "\n",
    "network.report_step = report_steps\n",
    "network.initial_learning_rate = initial_learning_rate\n",
    "network.decay = decay\n",
    "network.image_height = image_shape[1]\n",
    "network.image_width = image_shape[0]\n",
    "network.batch_size = batch_size\n",
    "\n",
    "network.class_sizes = size_of_classes\n",
    "network.num_labels = sum(network.class_sizes)\n",
    "\n",
    "network.makeGraph()\n",
    "\n",
    "# validation_data,validation_labels=dataMaker(9)\n",
    "# test_data,test_labels=dataMaker(100)\n",
    "# mf.showMultipleArraysHorizontally([test_data[i,0:28,0:140,0] for i in range(5)], test_labels,max_per_row=1)\n",
    "\n",
    "def dataMaker(batch_size):\n",
    "    gen = dataGenerator(batch_size,pickle_file)\n",
    "    while True:\n",
    "        data, labels = next(gen)\n",
    "        labels = sv.multipleOneHots(labels,size_of_classes)\n",
    "        yield data, labels\n",
    "\n",
    "    \n",
    "# test generator\n",
    "# --------------\n",
    "# generator = dataMaker(3)\n",
    "# sample_gen_data, sample_gen_label = next(generator)\n",
    "# print(sample_gen_label)\n",
    "# sv.showMultipleArraysHorizontally(sample_gen_data+.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.loss 15.5186\n",
      "Validation accuracy [ 41.25   6.25  11.25   7.5   10.  ]\n",
      "self.loss 6.79129\n",
      "Validation accuracy [ 51.25  15.    22.5   75.    98.75]\n",
      "self.loss 7.67373\n",
      "Validation accuracy [ 58.75  38.75  22.5   80.    95.  ]\n",
      "self.loss 5.80273\n",
      "Validation accuracy [ 56.25  25.    18.75  77.5   93.75]\n",
      "self.loss 6.26034\n",
      "Validation accuracy [ 53.75  35.    13.75  67.5   93.75]\n",
      "self.loss 6.96462\n",
      "Validation accuracy [  51.25   28.75   20.     71.25  100.  ]\n",
      "self.loss 6.90306\n",
      "Validation accuracy [ 48.75  30.    16.25  70.    97.5 ]\n",
      "self.loss 7.77455\n",
      "Validation accuracy [ 60.    21.25  21.25  80.    97.5 ]\n",
      "self.loss 6.6016\n",
      "Validation accuracy [ 53.75  25.    18.75  68.75  93.75]\n",
      "self.loss 6.77762\n",
      "Validation accuracy [ 61.25  25.     7.5   63.75  98.75]\n",
      "self.loss 8.27574\n",
      "Validation accuracy [ 57.5   30.    21.25  72.5   95.  ]\n",
      "Test accuracy: [ 59.     28.25   14.125  70.75   95.5  ]\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "# ===========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "validation_steps = 10\n",
    "test_steps = 100\n",
    "\n",
    "# train model\n",
    "generator = dataMaker(network.batch_size)\n",
    "prediction_sample = network.train(number_of_steps,generator,validation_steps,test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: model_report_2016-09-21_00:57:31\n"
     ]
    }
   ],
   "source": [
    "network.saveReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Check test results\n",
    "# ==================\n",
    "\n",
    "\n",
    "predicted_labels = oneHotsToLabels(prediction_sample[1],size_of_classes)\n",
    "print(predicted_labels)\n",
    "sv.showMultipleArraysHorizontally(prediction_sample[0]+.5,predicted_labels,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
