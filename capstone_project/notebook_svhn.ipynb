{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import svhnFileReader as sv\n",
    "import numpy as np\n",
    "import TrainSVHN as ts\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already exists\n",
      "test already exists\n",
      "extra already exists\n",
      "The random file indexes [114 342 143 252 321 268 146 394 177  45 173  39 372 340  16 209  55  37\n",
      " 384 275]\n",
      "Train files are ['115.png', '343.png', '144.png', '253.png', '322.png', '269.png', '147.png', '395.png', '178.png', '46.png', '174.png', '40.png', '373.png', '341.png', '17.png', '210.png', '56.png', '38.png', '385.png', '276.png']\n",
      "Label of train files are [[3.0, 2.0], [2.0], [1.0, 9.0, 5.0], [1.0, 9.0], [3.0, 1.0], [3.0, 5.0, 3.0], [1.0, 5.0], [1.0, 2.0, 9.0], [2.0, 1.0], [3.0, 2.0, 7.0], [5.0, 9.0], [2.0, 10.0], [3.0, 8.0], [2.0, 8.0, 9.0], [7.0, 9.0], [2.0, 6.0], [5.0, 6.0], [1.0, 7.0], [2.0, 6.0, 1.0], [1.0, 7.0, 8.0]]\n",
      "Shape of train data is (20, 50, 100, 3)\n",
      "Min and Max of data are 0.0 255.0\n",
      "Train labels are before parsing:\n",
      " [[3.0, 2.0], [2.0], [1.0, 9.0, 5.0], [1.0, 9.0], [3.0, 1.0], [3.0, 5.0, 3.0], [1.0, 5.0], [1.0, 2.0, 9.0], [2.0, 1.0], [3.0, 2.0, 7.0], [5.0, 9.0], [2.0, 10.0], [3.0, 8.0], [2.0, 8.0, 9.0], [7.0, 9.0]]\n",
      "Train labels are after parsing:\n",
      " [[ 2  3  2  0]\n",
      " [ 1  2  0  0]\n",
      " [ 3  1  9  5]\n",
      " [ 2  1  9  0]\n",
      " [ 2  3  1  0]\n",
      " [ 3  3  5  3]\n",
      " [ 2  1  5  0]\n",
      " [ 3  1  2  9]\n",
      " [ 2  2  1  0]\n",
      " [ 3  3  2  7]\n",
      " [ 2  5  9  0]\n",
      " [ 2  2 10  0]\n",
      " [ 2  3  8  0]\n",
      " [ 3  2  8  9]\n",
      " [ 2  7  9  0]]\n"
     ]
    }
   ],
   "source": [
    "# Dataset download and confirming data and functions\n",
    "# ==================================================\n",
    "\n",
    "# reload dependencies \n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "files = [\"train\", \"test\", \"extra\"]\n",
    "\n",
    "# download svhn files\n",
    "for file in files:\n",
    "    sv.maybeDownload(file)\n",
    "\n",
    "# choose some random indieces\n",
    "data_samples = np.random.permutation(400)[:20]\n",
    "print(\"The random file indexes\", data_samples) # Looking good?\n",
    "\n",
    "# get file names and labels associated with random indieces\n",
    "train_files, train_labels = sv.getLabels(\n",
    "    'train/digitStruct.mat', data_samples)\n",
    "\n",
    "# check the file names and labels\n",
    "print(\"Train files are\", train_files)\n",
    "print(\"Label of train files are\", train_labels)\n",
    "\n",
    "# get the data in file names\n",
    "data = sv.getImage(train_files, 'train/', shape=(80, 40))\n",
    "print(\"Shape of train data is\",data.shape) # what is the size? does it match?\n",
    "print(\"Min and Max of data are\", np.min(data), np.max(data))\n",
    "print(\"Train labels are before parsing:\\n\", train_labels[:15])\n",
    "print(\"Train labels are after parsing:\\n\", sv.parseLabels(train_labels[:15],3))\n",
    "\n",
    "# show some of the data\n",
    "sv.showMultipleArraysHorizontally(data[:15], train_labels[:15], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compeleted %5\n",
      "Compeleted %11\n",
      "Compeleted %17\n",
      "Compeleted %23\n",
      "Compeleted %29\n",
      "Compeleted %35\n",
      "Compeleted %41\n",
      "Compeleted %47\n",
      "Compeleted %53\n",
      "Compeleted %59\n",
      "Compeleted %65\n",
      "Compeleted %71\n",
      "Compeleted %77\n",
      "Compeleted %83\n",
      "Compeleted %89\n",
      "Compeleted %95\n",
      "Compeleted %101\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# =============\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "# configurations\n",
    "dataset = 'train'\n",
    "big_batch_size = 2000\n",
    "image_shape = (80,40)\n",
    "max_digits_in_label = 4\n",
    "pickle_file = dataset+\"_preprocessed\"\n",
    "\n",
    "def preprocess():\n",
    "    # read lots of files\n",
    "    struct_file = dataset+\"/digitStruct.mat\"\n",
    "    number_of_files = sv.getNumberOfFiles(struct_file)\n",
    "#     number_of_files = big_batch_size # just for debug\n",
    "    data_samples = np.random.permutation(number_of_files)\n",
    "    file_handle = open(pickle_file,\"wb\")\n",
    "\n",
    "    # iterate over data in big batches\n",
    "    for batch_start in range(0,number_of_files, big_batch_size):\n",
    "\n",
    "        # read the .mat file and parse attributes of data files\n",
    "        batch_indexes = data_samples[batch_start:batch_start+big_batch_size]\n",
    "\n",
    "        file_names,train_labels = sv.getLabels(struct_file,batch_indexes)\n",
    "        train_values = sv.getImage(file_names, dataset,shape=image_shape)\n",
    "\n",
    "\n",
    "        # form and normalize\n",
    "        pixel_depth = 255\n",
    "        train_values = sv.scaleData(train_values,pixel_depth)\n",
    "        train_labels = sv.parseLabels(train_labels,max_digits_in_label)\n",
    "\n",
    "        # save in file\n",
    "        np.save(file_handle, train_values)\n",
    "        np.save(file_handle, train_labels)\n",
    "\n",
    "        # process status\n",
    "        completion_percentil = 100*(batch_start+big_batch_size)/number_of_files\n",
    "        print(\"Compeleted %%%d\"%completion_percentil)\n",
    "\n",
    "    # always close the file\n",
    "    file_handle.close()\n",
    "    \n",
    "# perform preprocessing\n",
    "preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data extractor\n",
    "# ==============\n",
    "\n",
    "def dataGenerator(batch_size,file_name):\n",
    "    file_handle = open(file_name, \"rb\")\n",
    "    while True:\n",
    "\n",
    "        # get data array\n",
    "        try:\n",
    "            data = np.load(file_handle)\n",
    "        # if reached end of file\n",
    "        except OSError:\n",
    "#             print(\"in dataGenerator() pointer is at\",file_handle.tell(),\"... going back.\")\n",
    "            # go to the beginning\n",
    "            file_handle.seek(0)\n",
    "            # and try loading again\n",
    "            data = np.load(file_handle)\n",
    "\n",
    "        # get label array\n",
    "        labels = np.load(file_handle)\n",
    "        \n",
    "        # randomize\n",
    "        data,labels = sv.shuffleArrays([data,labels])\n",
    "        \n",
    "        # get batches        \n",
    "        number_of_datapoints = labels.shape[0]\n",
    "        full_batches = number_of_datapoints//batch_size # few datapoints are going to waste here\n",
    "        start_point = 0\n",
    "        for batch_start in range(0,full_batches,batch_size):\n",
    "            batch_data = data[batch_start:batch_start+batch_size]\n",
    "            batch_labels = labels[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # yield both\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate dataGenerator and disk data\n",
    "# ====================================\n",
    "\n",
    "importlib.reload(sv)\n",
    "\n",
    "gen = dataGenerator(3,pickle_file)\n",
    "sample_data,sample_labels = next(gen)\n",
    "\n",
    "print(sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 5 0 0]\n",
      " [2 1 9 0 0]\n",
      " [2 2 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def oneHotsToLabels(onehots,class_sizes):\n",
    "    offset=0\n",
    "    labels = np.zeros((len(onehots),len(class_sizes)),int)\n",
    "    for i in range(len(class_sizes)):\n",
    "        labels[:,i]=np.argmax(onehots[:,offset:offset+class_sizes[i]],1)\n",
    "        offset+=class_sizes[i]\n",
    "    return labels\n",
    "\n",
    "one_hots = sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label)\n",
    "print(oneHotsToLabels(one_hots,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "\n",
    "sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape is [8, 49]\n",
      "Seperated shapes are [[8, 5], [8, 11], [8, 11], [8, 11], [8, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "# ==========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "initial_learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "\n",
    "# model\n",
    "size_of_classes = [max_digits_in_label+1]+[11]*max_digits_in_label\n",
    "network = ts.SVHNTrainer()\n",
    "network.initial_learning_rate = initial_learning_rate\n",
    "network.image_height = image_shape[1]\n",
    "network.image_width = image_shape[0]\n",
    "network.batch_size = batch_size\n",
    "network.class_sizes = size_of_classes\n",
    "network.num_labels = sum(network.class_sizes)\n",
    "network.makeGraph()\n",
    "\n",
    "# validation_data,validation_labels=dataMaker(9)\n",
    "# test_data,test_labels=dataMaker(100)\n",
    "# mf.showMultipleArraysHorizontally([test_data[i,0:28,0:140,0] for i in range(5)], test_labels,max_per_row=1)\n",
    "\n",
    "def dataMaker(batch_size):\n",
    "    gen = dataGenerator(batch_size,pickle_file)\n",
    "    while True:\n",
    "        data, labels = next(gen)\n",
    "        labels = sv.multipleOneHots(labels,size_of_classes)\n",
    "        yield data, labels\n",
    "\n",
    "    \n",
    "# test generator\n",
    "# --------------\n",
    "# generator = dataMaker(3)\n",
    "# sample_gen_data, sample_gen_label = next(generator)\n",
    "# print(sample_gen_label)\n",
    "# sv.showMultipleArraysHorizontally(sample_gen_data+.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.loss 29.9678\n",
      "Validation accuracy [ 17.5    3.75  11.25   3.75  30.  ]\n",
      "self.loss 7.99489\n",
      "Validation accuracy [ 52.5    5.     8.75  60.    92.5 ]\n",
      "self.loss 7.53297\n",
      "Validation accuracy [ 57.5   16.25  15.    73.75  92.5 ]\n",
      "self.loss 7.78253\n",
      "Validation accuracy [ 56.25  20.     8.75  68.75  95.  ]\n",
      "self.loss 9.40606\n",
      "Validation accuracy [ 60.    27.5   15.    78.75  98.75]\n",
      "self.loss 7.57804\n",
      "Validation accuracy [ 53.75  25.    17.5   72.5   96.25]\n",
      "self.loss 8.58639\n",
      "Validation accuracy [ 45.    33.75  13.75  65.    92.5 ]\n",
      "self.loss 7.75013\n",
      "Validation accuracy [ 52.5   21.25  15.    71.25  98.75]\n",
      "self.loss 8.51332\n",
      "Validation accuracy [ 50.    25.    15.    66.25  93.75]\n",
      "self.loss 8.14799\n",
      "Validation accuracy [ 61.25  32.5   13.75  72.5   93.75]\n",
      "self.loss 6.70943\n",
      "Validation accuracy [ 52.5   17.5   12.5   65.    96.25]\n",
      "Test accuracy: [ 55.     24.875  13.875  69.25   96.75 ]\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "# ===========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "number_of_steps = 1000\n",
    "validation_steps = 10\n",
    "test_steps = 100\n",
    "network.report_step = number_of_steps/10\n",
    "\n",
    "# train model\n",
    "generator = dataMaker(network.batch_size)\n",
    "prediction_sample = network.train(number_of_steps,generator,validation_steps,test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [2 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [2 0 0 0 0]\n",
      " [2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Check test results\n",
    "# ==================\n",
    "\n",
    "\n",
    "predicted_labels = oneHotsToLabels(prediction_sample[1],size_of_classes)\n",
    "print(predicted_labels)\n",
    "sv.showMultipleArraysHorizontally(prediction_sample[0]+.5,predicted_labels,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
