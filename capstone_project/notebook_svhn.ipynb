{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import svhnFileReader as sv\n",
    "import numpy as np\n",
    "import TrainSVHN as ts\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already exists\n",
      "test already exists\n",
      "extra already exists\n",
      "[159  84 266 168  10 101 265 156 224  85 173  70  44 275 160 207 232 305\n",
      " 134 105]\n",
      "['160.png', '85.png', '267.png', '169.png', '11.png', '102.png', '266.png', '157.png', '225.png', '86.png', '174.png', '71.png', '45.png', '276.png', '161.png', '208.png', '233.png', '306.png', '135.png', '106.png']\n",
      "[[2.0, 10.0, 8.0], [1.0, 7.0], [2.0, 1.0], [4.0, 2.0], [2.0, 3.0], [3.0, 5.0], [5.0, 10.0], [5.0, 7.0], [2.0], [3.0, 5.0], [5.0, 9.0], [2.0, 3.0, 8.0, 6.0], [1.0, 10.0], [1.0, 7.0, 8.0], [8.0, 5.0], [9.0], [1.0, 3.0, 5.0, 8.0], [8.0, 7.0], [6.0, 8.0], [1.0, 4.0]]\n",
      "(20, 50, 100, 3)\n",
      "[0.0, 2.0, 10.0]\n"
     ]
    }
   ],
   "source": [
    "# Dataset download and evaluation\n",
    "# ===============================\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "files = [\"train\", \"test\", \"extra\"]\n",
    "\n",
    "# download svhn files\n",
    "for file in files:\n",
    "    sv.maybeDownload(file)\n",
    "\n",
    "# choose some random indieces\n",
    "data_samples = np.random.randint(1, 400, size=20)\n",
    "print(data_samples) # are they random?\n",
    "\n",
    "# get file names and labels associated with random indieces\n",
    "train_files, train_labels = sv.getLabels(\n",
    "    'train/digitStruct.mat', data_samples)\n",
    "\n",
    "# check the file names and labels\n",
    "print(train_files)\n",
    "print(train_labels)\n",
    "\n",
    "# get the data in file names\n",
    "data = sv.getImage(train_files, 'train/', shape=(100, 50))\n",
    "print(data.shape) # what is the size? does it match?\n",
    "print(sv.parseLabel(train_labels[0]))\n",
    "\n",
    "# show some of the data\n",
    "sv.showMultipleArraysHorizontally(data[:15], train_labels[:15], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compeleted 200 out of 33402\n",
      "Compeleted 400 out of 33402\n",
      "Compeleted 600 out of 33402\n",
      "Compeleted 800 out of 33402\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-cf8e6c727be9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# save in file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[1;32m--> 491\u001b[1;33m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    492\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mwrite_array\u001b[1;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m             \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "# =============\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "# configurations\n",
    "dataset = 'train'\n",
    "big_batch_size = 200\n",
    "image_shape = (100,50)\n",
    "pickle_file = dataset+\"_preprocessed\"\n",
    "\n",
    "# read lots of files\n",
    "struct_file = dataset+\"/digitStruct.mat\"\n",
    "number_of_files = sv.getNumberOfFiles(struct_file)\n",
    "data_samples = np.random.randint(0, number_of_files, number_of_files)\n",
    "file_handle = open(pickle_file,\"wb\")\n",
    "\n",
    "# iterate over data in big batches\n",
    "for batch_start in range(0,number_of_files, big_batch_size):\n",
    "    \n",
    "    # read the .mat file and parse attributes of data files\n",
    "    batch_indexes = data_samples[batch_start:batch_start+big_batch_size]\n",
    "    print(\"Compeleted %d out of %d\"%(batch_start+big_batch_size,number_of_files))\n",
    "\n",
    "    file_names,train_labels = sv.getLabels(struct_file,batch_indexes)\n",
    "    train_values = sv.getImage(file_names, dataset,shape=image_shape)\n",
    "\n",
    "    # form and normalize\n",
    "    pixel_depth = 255\n",
    "    train_values = sv.scaleData(train_values,pixel_depth)\n",
    "#     train_labels = sv.formLabels(train_labels)\n",
    "    \n",
    "    \n",
    "    # save in file\n",
    "    np.save(file_handle, train_values)\n",
    "    np.save(file_handle, train_labels)\n",
    "    \n",
    "# always close the file\n",
    "file_handle.close()\n",
    "\n",
    "\n",
    "# Define generator\n",
    "# ================\n",
    "def generator(batch_size,file_name):\n",
    "    file_handle = open(file_name, \"rb\")\n",
    "    while True:\n",
    "\n",
    "        # get data array\n",
    "        try:\n",
    "            data = np.load(file_handle)\n",
    "        # if reached end of file\n",
    "        except OSError:\n",
    "            print(\"Pointer is at\",file_handle.tell())\n",
    "            # go to the beginning\n",
    "            file_handle.seek(0)\n",
    "            # and try loading again\n",
    "            data = np.load(file_handle)\n",
    "\n",
    "        # get label array\n",
    "        labels = np.load(file_handle)\n",
    "        \n",
    "        # randomize\n",
    "        number_of_datapoints = labels.shape[0]\n",
    "        random_indexes = np.random.randint(1, number_of_datapoints, number_of_datapoints)\n",
    "        data = data[random_indexes]\n",
    "        labels = labels[random_indexes]\n",
    "        del random_indexes\n",
    "        \n",
    "        # get batches        \n",
    "        full_batches = number_of_datapoints//batch_size # few datapoints are going to waste here\n",
    "        start_point = 0\n",
    "        for batch_start in range(0,full_batches,batch_size):\n",
    "            batch_data = data[batch_start:batch_start+batch_size]\n",
    "            batch_labels = labels[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # yield both\n",
    "            yield batch_data, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape is [8, 56]\n",
      "Seperated shapes are [[8, 6], [8, 10], [8, 10], [8, 10], [8, 10], [8, 10]]\n",
      "loss shape is ()\n",
      "(9, 50, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "importlib.reload(ts)\n",
    "network = ts.SVHNTrainer()\n",
    "network.makeGraph()\n",
    "\n",
    "def dataMaker(batch_size,dataset=\"train\"):\n",
    "    data_samples=np.random.randint(0,400,size=batch_size)\n",
    "    train_files, train_labels = sv.getLabels(dataset+\"/digitStruct.mat\", data_samples)\n",
    "    data = sv.getImage(train_files, 'train', shape=image_shape)\n",
    "#     batch_image, batch_labels = mf.fixedSizeMultipleNumberRows(train_data,train_labels,digit_length+1,5)\n",
    "    print(data.shape)\n",
    "    batch_image = sv.scaleData(np.expand_dims(data,3),255)\n",
    "    return batch_image,sv.toOnehot([len(label) for label in train_labels],5)\n",
    "\n",
    "validation_data,validation_labels=dataMaker(9)\n",
    "# test_data,test_labels=dataMaker(100)\n",
    "# mf.showMultipleArraysHorizontally([test_data[i,0:28,0:140,0] for i in range(5)], test_labels,max_per_row=1)\n",
    "\n",
    "# network.train(10000,dataMaker,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
