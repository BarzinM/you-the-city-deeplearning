{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import svhnFileReader as sv\n",
    "import numpy as np\n",
    "import TrainSVHN as ts\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already exists\n",
      "test already exists\n",
      "extra already exists\n",
      "The random file indexes [370  94  80  72  84  81 260   1 287  51 167 263 180  48 274  42  67  68\n",
      " 330 123]\n",
      "Train files are ['371.png', '95.png', '81.png', '73.png', '85.png', '82.png', '261.png', '2.png', '288.png', '52.png', '168.png', '264.png', '181.png', '49.png', '275.png', '43.png', '68.png', '69.png', '331.png', '124.png']\n",
      "Label of train files are [[8.0], [1.0, 10.0, 4.0, 4.0], [7.0, 4.0], [3.0], [1.0, 7.0], [4.0, 5.0], [1.0, 4.0], [2.0, 3.0], [2.0, 9.0, 2.0], [3.0, 5.0], [2.0, 8.0, 2.0], [2.0, 1.0, 9.0, 10.0], [3.0, 7.0], [8.0, 1.0, 7.0], [2.0, 5.0], [2.0, 9.0], [5.0, 4.0, 2.0], [4.0, 4.0], [2.0, 3.0], [5.0]]\n",
      "Shape of train data is (20, 40, 80, 3)\n",
      "Min and Max of data are 0.0 255.0\n",
      "Train labels are before parsing:\n",
      " [[8.0], [1.0, 10.0, 4.0, 4.0], [7.0, 4.0], [3.0], [1.0, 7.0], [4.0, 5.0], [1.0, 4.0], [2.0, 3.0], [2.0, 9.0, 2.0], [3.0, 5.0], [2.0, 8.0, 2.0], [2.0, 1.0, 9.0, 10.0], [3.0, 7.0], [8.0, 1.0, 7.0], [2.0, 5.0]]\n",
      "Train labels are after parsing:\n",
      " [[ 1  8  0  0]\n",
      " [ 0  1 10  4]\n",
      " [ 2  7  4  0]\n",
      " [ 1  3  0  0]\n",
      " [ 2  1  7  0]\n",
      " [ 2  4  5  0]\n",
      " [ 2  1  4  0]\n",
      " [ 2  2  3  0]\n",
      " [ 3  2  9  2]\n",
      " [ 2  3  5  0]\n",
      " [ 3  2  8  2]\n",
      " [ 0  2  1  9]\n",
      " [ 2  3  7  0]\n",
      " [ 3  8  1  7]\n",
      " [ 2  2  5  0]]\n"
     ]
    }
   ],
   "source": [
    "# Dataset download and confirming data and functions\n",
    "# ==================================================\n",
    "\n",
    "# reload dependencies \n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "files = [\"train\", \"test\", \"extra\"]\n",
    "\n",
    "# download svhn files\n",
    "for file in files:\n",
    "    sv.maybeDownload(file)\n",
    "\n",
    "# choose some random indieces\n",
    "data_samples = np.random.permutation(400)[:20]\n",
    "print(\"The random file indexes\", data_samples) # Looking good?\n",
    "\n",
    "# get file names and labels associated with random indieces\n",
    "train_files, train_labels = sv.getLabels(\n",
    "    'train/digitStruct.mat', data_samples)\n",
    "\n",
    "# check the file names and labels\n",
    "print(\"Train files are\", train_files)\n",
    "print(\"Label of train files are\", train_labels)\n",
    "\n",
    "# get the data in file names\n",
    "data = sv.getImage(train_files, 'train/', shape=(80, 40))\n",
    "print(\"Shape of train data is\",data.shape) # what is the size? does it match?\n",
    "print(\"Min and Max of data are\", np.min(data), np.max(data))\n",
    "print(\"Train labels are before parsing:\\n\", train_labels[:15])\n",
    "print(\"Train labels are after parsing:\\n\", sv.parseLabels(train_labels[:15],3))\n",
    "\n",
    "# show some of the data\n",
    "# sv.showMultipleArraysHorizontally(data[:15], train_labels[:15], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# =============\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "importlib.reload(sv)\n",
    "\n",
    "# configurations\n",
    "big_batch_size = 2000\n",
    "image_shape = (80,40)\n",
    "max_digits_in_label = 4\n",
    "\n",
    "def preprocess(dataset):\n",
    "    # read lots of files\n",
    "    pickle_file = dataset+\"_preprocessed\"\n",
    "    struct_file = dataset+\"/digitStruct.mat\"\n",
    "    number_of_files = sv.getNumberOfFiles(struct_file)\n",
    "#     number_of_files = big_batch_size # just for debug\n",
    "    data_samples = np.random.permutation(number_of_files)\n",
    "    file_handle = open(pickle_file,\"wb\")\n",
    "\n",
    "    # iterate over data in big batches\n",
    "    for batch_start in range(0,number_of_files, big_batch_size):\n",
    "\n",
    "        # read the .mat file and parse attributes of data files\n",
    "        batch_indexes = data_samples[batch_start:batch_start+big_batch_size]\n",
    "\n",
    "        file_names,train_labels = sv.getLabels(struct_file,batch_indexes)\n",
    "        train_values = sv.getImage(file_names, dataset,shape=image_shape)\n",
    "\n",
    "\n",
    "        # form and normalize\n",
    "        pixel_depth = 255\n",
    "        train_values = sv.scaleData(train_values,pixel_depth)\n",
    "        train_labels = sv.parseLabels(train_labels,max_digits_in_label)\n",
    "\n",
    "        # save in file\n",
    "        np.save(file_handle, train_values)\n",
    "        np.save(file_handle, train_labels)\n",
    "\n",
    "        # process status\n",
    "        completion_percentil = 100*(batch_start+big_batch_size)/number_of_files\n",
    "        print(\"Compeleted %%%d\"%completion_percentil)\n",
    "\n",
    "    # always close the file\n",
    "    file_handle.close()\n",
    "    \n",
    "# perform preprocessing\n",
    "# preprocess('train')\n",
    "# preprocess('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data extractor\n",
    "# ==============\n",
    "\n",
    "def dataGenerator(batch_size,file_name):\n",
    "    file_handle = open(file_name, \"rb\")\n",
    "    while True:\n",
    "\n",
    "        # get data array\n",
    "        try:\n",
    "            data = np.load(file_handle)\n",
    "        # if reached end of file\n",
    "        except OSError:\n",
    "#             print(\"in dataGenerator() pointer is at\",file_handle.tell(),\"... going back.\")\n",
    "            # go to the beginning\n",
    "            file_handle.seek(0)\n",
    "            # and try loading again\n",
    "            data = np.load(file_handle)\n",
    "\n",
    "        # get label array\n",
    "        labels = np.load(file_handle)\n",
    "        \n",
    "        # randomize\n",
    "        data,labels = sv.shuffleArrays([data,labels])\n",
    "        \n",
    "        # get batches        \n",
    "        number_of_datapoints = labels.shape[0]\n",
    "        full_batches = number_of_datapoints//batch_size # few datapoints are going to waste here\n",
    "        start_point = 0\n",
    "        for batch_start in range(0,full_batches,batch_size):\n",
    "            batch_data = data[batch_start:batch_start+batch_size]\n",
    "            batch_labels = labels[batch_start:batch_start+batch_size]\n",
    "            \n",
    "            # yield both\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate dataGenerator and disk data\n",
    "# ====================================\n",
    "\n",
    "importlib.reload(sv)\n",
    "\n",
    "train_file = \"train_preprocessed\"\n",
    "gen = dataGenerator(3,train_file)\n",
    "sample_data,sample_labels = next(gen)\n",
    "\n",
    "print(sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "# sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)\n",
    "del gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 1 0 0]\n",
      " [2 3 4 0 0]\n",
      " [2 3 4 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def oneHotsToLabels(onehots,class_sizes):\n",
    "    offset=0\n",
    "    labels = np.zeros((len(onehots),len(class_sizes)),int)\n",
    "    for i in range(len(class_sizes)):\n",
    "        labels[:,i]=np.argmax(onehots[:,offset:offset+class_sizes[i]],1)\n",
    "        offset+=class_sizes[i]\n",
    "    return labels\n",
    "\n",
    "one_hots = sv.multipleOneHots(sample_labels,[max_digits_in_label+1]+[11]*max_digits_in_label)\n",
    "print(oneHotsToLabels(one_hots,[max_digits_in_label+1]+[11]*max_digits_in_label))\n",
    "\n",
    "# sv.showMultipleArraysHorizontally(sample_data+.5,sample_labels,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_channels': 3, 'report_step': 200, 'class_sizes': [5, 11, 11, 11, 11], 'max_pool_strides': [2, 1, 2, 1, 2, 1], 'image_width': 80, 'image_height': 40, 'depth_conv': [48, 64, 128, 160, 192, 192], 'graph': <tensorflow.python.framework.ops.Graph object at 0x7f8e1f9e2208>, 'batch_size': 8, 'depth_fully_connected': [1000, 1000], 'report_string': '----------', 'output_neurons': 49, 'decay': 0.9, 'initial_learning_rate': 0.001}\n",
      "Weights of convolution layers have the following sizes:\n",
      "(5, 5, 3, 48) maxpool stride: 2\n",
      "(5, 5, 48, 64) maxpool stride: 1\n",
      "(5, 5, 64, 128) maxpool stride: 2\n",
      "(5, 5, 128, 160) maxpool stride: 1\n",
      "(5, 5, 160, 192) maxpool stride: 2\n",
      "(5, 5, 192, 192) maxpool stride: 1\n",
      "Data has the following shapes between convolutional layers:\n",
      "(8, 40, 80, 3)\n",
      "(8, 20, 40, 48)\n",
      "(8, 20, 40, 64)\n",
      "(8, 10, 20, 128)\n",
      "(8, 10, 20, 160)\n",
      "(8, 5, 10, 192)\n",
      "Data's shape before flat layers is: (8, 9600)\n",
      "Data has following shapes between flat layers:\n",
      "(8, 1000)\n",
      "(8, 1000)\n",
      "Final shape of data is: (8, 49)\n",
      "Seperated shapes are [[8, 5], [8, 11], [8, 11], [8, 11], [8, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "# ==========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "number_of_steps = 100\n",
    "batch_size = 8\n",
    "depth_conv = [48, 64]\n",
    "strides = [2,1]\n",
    "depth_fully_connected = [10, 10]\n",
    "initial_learning_rate = 1e-3\n",
    "decay = .9\n",
    "report_steps = number_of_steps // 10\n",
    "size_of_classes = [max_digits_in_label+1]+[11]*max_digits_in_label\n",
    "\n",
    "\n",
    "# model\n",
    "network = ts.SVHNTrainer()\n",
    "\n",
    "# apply config\n",
    "network.depth_conv = depth_conv\n",
    "network.max_pool_strides = strides\n",
    "network.depth_fully_connected = depth_fully_connected\n",
    "network.num_channels = 3\n",
    "network.report_step = report_steps\n",
    "network.initial_learning_rate = initial_learning_rate\n",
    "network.decay = decay\n",
    "network.image_height = image_shape[1]\n",
    "network.image_width = image_shape[0]\n",
    "network.batch_size = batch_size\n",
    "network.class_sizes = size_of_classes\n",
    "network.output_neurons = sum(network.class_sizes)\n",
    "\n",
    "network.makeGraph()\n",
    "\n",
    "# define the generator\n",
    "def dataMaker(batch_size):\n",
    "    gen = dataGenerator(batch_size,train_file)\n",
    "    while True:\n",
    "        data, labels = next(gen)\n",
    "        labels = sv.multipleOneHots(labels,size_of_classes)\n",
    "        yield data, labels\n",
    "\n",
    "    \n",
    "# test generator\n",
    "# --------------\n",
    "# generator = dataMaker(3)\n",
    "# sample_gen_data, sample_gen_label = next(generator)\n",
    "# print(sample_gen_label)\n",
    "# sv.showMultipleArraysHorizontally(sample_gen_data+.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "# ===========\n",
    "\n",
    "# dependencies\n",
    "importlib.reload(ts)\n",
    "\n",
    "# configurations\n",
    "validation_steps = 10\n",
    "test_steps = 100\n",
    "\n",
    "# train model\n",
    "del generator\n",
    "generator = dataMaker(network.batch_size)\n",
    "prediction_sample = network.train(number_of_steps,generator,validation_steps,test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: model_report_2016-09-21_15:22:23\n"
     ]
    }
   ],
   "source": [
    "network.saveReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]\n",
      " [2 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Check test results\n",
    "# ==================\n",
    "\n",
    "\n",
    "predicted_labels = oneHotsToLabels(prediction_sample[1],size_of_classes)\n",
    "print(predicted_labels)\n",
    "sv.showMultipleArraysHorizontally(prediction_sample[0]+.5,predicted_labels,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
